{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "*Christiaan M. Erwich*\n",
    "\n",
    "This notebook works with the matrices produced by the 'levenshtein_pgn_shifts_part1' notebook (which is still under construction for cleaner code, publication on Github will follow). The basic idea is to find parallel verses in the Hebrew Bible for all verses in the poetry of the Psalms on the basis of participant information, and calculate their similarity ratio (i.e. an evaluation of sameness) with the [Longest Common Subsequence](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) method. For the calculation of the similarity ratio the [Levenshtein python module](http://www.coli.uni-saarland.de/courses/LT1/2011/slides/Python-Levenshtein.html) was used. \n",
    "\n",
    "The matrices (pickle files) produced by part1 can be found in this folder. The three notebooks are part of my research project 'Who is Who in the Psalms: A Computational Analysis of Participants and Their Networks.', funded by NWO. Part of the project is to find patterns of participant shifts (i.e. shifts in person, number, gender) in the Psalms, to enable identification of those participants. The PNG-patterns generated with this notebook are an experimental starting point for the identification of participants in the Psalms. \n",
    "\n",
    "The notebooks were used for a presentation that I gave at the \"Plotting Poetry: On Mechanically Enhanced Reading\" conference in Basel on October 6th 2017. The presentation can be downloaded from this folder as well. \n",
    "\n",
    "Most of the code in this notebook is borrowed from two notebooks written by Dirk Roorda:\n",
    "\n",
    "\n",
    "* [Kings and parallels](https://github.com/ETCBC/parallels/blob/master/programs/kings_ii.ipynb/)\n",
    "* [Parallels](https://github.com/ETCBC/parallels/blob/master/programs/parallels.ipynb/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The program\n",
    "The program starts in the next cell, with the loading of several modules that are important for the analysis of the Psalms and their participant patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, re, pickle\n",
    "import collections, difflib\n",
    "from Levenshtein import ratio\n",
    "\n",
    "# (sudo -H) pip(3) install matplotlib\n",
    "\n",
    "from IPython.display import HTML, display_pretty, display_html\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from random import random\n",
    "\n",
    "from pprint import pprint\n",
    "from tf.fabric import Fabric\n",
    "from tf.transcription import Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data source\n",
    "The ETCBC database is used in version 4c. Downloadable from the GitHub repo [text-fabric-data](https://github.com/ETCBC/bhsa). The format of the data obtained through Github is immediately ready to be used by Text-Fabric, and hence by this notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.0.3\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "107 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "source = 'etcbc'\n",
    "version = '4c'\n",
    "ETCBC = 'hebrew/{}{}'.format(source, version)\n",
    "\n",
    "DATABASE = '~/github/etcbc'\n",
    "BHSA = 'bhsa/tf/2016'\n",
    "TF = Fabric(locations=[DATABASE], modules=[BHSA], silent=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data features\n",
    "Some features from the ETCBC database are used. You see them in the code below. Their documentation can be found through the SHEBANQ help function or via this direct link: [insert link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.00s Feature overview: 102 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "  0.07s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    otype\n",
    "    lex lex_utf8 g_word_utf8 trailer_utf8\n",
    "    book chapter verse label number\n",
    "    nu ps gn vs vt prs ls lex g_cons\n",
    "    function txt domain rela code gloss\n",
    "    sp kind typ pdp language\n",
    "''')\n",
    "\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration information\n",
    "Here the constants that refer to the files we read and write are found. These are the results of part1. The notebook has to be told where to find its results.\n",
    "CHUNK_GREP, MATRIX_GREP and SIM_THRESHOLD_GREP define the similarity method by which we grab the verses that are similar to verses in the Psalms\n",
    "Throughout this notebook English names for the books of the Bible are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the language of the book names\n",
    "LANG = 'en'\n",
    "\n",
    "# the book and chapters that are central to our study\n",
    "REFBOOKS = {'Psalms'} # Psalms\n",
    "REFCHAPTERS = set(range(89,90)) # Psalms 89\n",
    "\n",
    "\n",
    "TF_OUTPUT = os.path.expanduser('~/text-fabric-output')\n",
    "# the results of the parallel notebook.\n",
    "CROSSREF_APP = 'parallels'\n",
    "# directory of computed intermediary results of parallel.\n",
    "PRECOMP_DIR = '{}/{}{}/{}/{}'.format(TF_OUTPUT, source, version, CROSSREF_APP, 'stored')\n",
    "# precomputed list of verse chunks\n",
    "CHUNK_GREP = '{}/chunks/chunk_{}_{}'.format(PRECOMP_DIR, 'O', 'verse') \n",
    "# precomputed matrix of similarities based on verse chunks and the LCS method\n",
    "MATRIX_GREP = '{}/matrices/matrix_{}_{}_{}_{}'.format(PRECOMP_DIR, 'O', 'verse', 'LCS', 75)\n",
    "\n",
    "MATRIX_GREP_PATTERNS_VERSE = '{}/matrices/matrix_{}_{}_{}_{}_withpatterns'.format(PRECOMP_DIR, 'O', 'verse', 'LCS', 75)\n",
    "\n",
    "\n",
    "# the similarity threshold above which we consider verses similar\n",
    "SIM_THRESHOLD_GREP = 75\n",
    "\n",
    "# output files\n",
    "NCOL_FILE = 'psalms_crossrefs.ncol'           # graph of similar verses to Psalms \n",
    "SIMILAR_FILE = 'psalms_similarities.tsv'      # refined similarities based on sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Book name index\n",
    "\n",
    "Book of interest is the Psalms, which is analysed seperately from the other books. Therefore a virtual book 'Psalmsr' is introduced for the reference chapters that we want to study. \n",
    "The ETCBC database uses Latin names for the Bible books. For ease of referencee, they are translated to conventional English names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_node = dict()\n",
    "for b in F.otype.s('book'):\n",
    "    book_name = T.bookName(b, lang=LANG)\n",
    "    book_node[book_name] = b\n",
    "    if book_name == 'Psalms': \n",
    "        book_node[book_name+'r'] = b\n",
    "\n",
    "def passage_key(p):\n",
    "    (bk, ch, vs) = p\n",
    "    return (-1, ch, vs) if bk in REFBOOKS and ch in REFCHAPTERS else (book_node[bk], ch, vs)\n",
    "\n",
    "# the format of verse references\n",
    "PASSAGE_FMT = '{}~{}:{}'\n",
    "PASSAGER_FMT = '{}r~{}:{}' # used for the pseudo-book Psalmsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parallels within the Masoretic Text\n",
    "## 2.1 Grep all parallel verses\n",
    "\n",
    "The aim is to find the verses that are similar to any verse in the Psalms. One of the similarity matrices that has been computed by the parallels notebook is used. To be precise, we took the matrix computed for the LCS method applied to verses. The similarities higher than 75 are then extracted. These specifications are stored in the variables CHUNK_GREP, MATRIX_GREP and  SIM_THRESHOLD_GREP. Every similarity that is found is a pair of verse references, at least one of which is in the Psalms/reference chapter. That reference is rendered as being in the book Psalmsr because in a later visualisation the focus book/chapter is placed in a separate colummn.\n",
    "\n",
    "If you don't want to visualise the data, skip 2.3-2-4 and run all cells from 3. onwards. If you have already run 2.3-2-4, make sure you save the notebook, resart it and then run the cells from 3. onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run the cell below for the network visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(CHUNK_GREP, 'rb') as f: chunks = pickle.load(f)\n",
    "with open(MATRIX_GREP, 'rb') as f: grep_dist = pickle.load(f)\n",
    "\n",
    "def get_verse_ref(chunk):\n",
    "    sec = T.sectionFromNode(chunks[chunk][0], lang=LANG)\n",
    "    vn = T.nodeFromSection(sec, lang=LANG)\n",
    "    return (vn, sec)\n",
    "\n",
    "all_verse_nodes = set()\n",
    "n_internal = 0\n",
    "x = 0\n",
    "crossrefs = set()\n",
    "for ((c1, c2), r) in grep_dist.items():\n",
    "    if r < SIM_THRESHOLD_GREP: continue\n",
    "    (v1, (bk1, ch1, vs1)) = get_verse_ref(c1)\n",
    "    (v2, (bk2, ch2, vs2)) = get_verse_ref(c2)\n",
    "    # remove comments for exluding internal references\n",
    "    \n",
    "    #if bk1 in REFBOOKS and ch1 in REFCHAPTERS and bk2 in REFBOOKS and ch2 in REFCHAPTERS:\n",
    "        #n_internal += 1\n",
    "        #continue\n",
    "    if bk1 in REFBOOKS and ch1 in REFCHAPTERS:\n",
    "        bkx = bk1\n",
    "        chx = ch1\n",
    "        vsx = vs1\n",
    "        bky = bk2\n",
    "        chy = ch2\n",
    "        vsy = vs2\n",
    "    elif bk2 in REFBOOKS and ch2 in REFCHAPTERS:\n",
    "        bkx = bk2\n",
    "        chx = ch2\n",
    "        vsx = vs2\n",
    "        bky = bk1\n",
    "        chy = ch1\n",
    "        vsy = vs1\n",
    "    else:\n",
    "        continue\n",
    "    crossrefs.add(((bkx, chx, vsx), (bky, chy, vsy), r))\n",
    "    all_verse_nodes |= {v1, v2}\n",
    "\n",
    "info('{} external crossrefs saved; {} internal crossrefs skipped; from total {} crossrefs'.format(\n",
    "    len(crossrefs), n_internal, len(grep_dist),\n",
    "))\n",
    "\n",
    "print('\\n'.join('{}r\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}'.format(*x[0], *x[1], round(x[2])) for x in sorted(crossrefs)[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Store similarities as a graph\n",
    "\n",
    "The similarities found so far are now visualised as a graph, where the verses are nodes and the similarities are edges. To that end the similarities are stored in a format such that graph software can read it.\n",
    "\n",
    "We write out the graph data as a file in '.ncol' format, and we will use the python package networkx to read and process that file.\n",
    "\n",
    "We also produce:\n",
    "* a set of all verses encountered\n",
    "* a set of all chapters encountered\n",
    "* a set of all books encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-cbeb8eea0af5>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-cbeb8eea0af5>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    print(' '.join(sorted(all_books)))-\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "info('Exporting graph info, assembling sets')\n",
    "ncolfile = open(NCOL_FILE, 'w')\n",
    "for (x, y, r) in sorted(crossrefs, key=lambda z: (\n",
    "        book_node[z[0][0]], z[0][1], z[0][2], \n",
    "        book_node[z[1][0]], z[1][1], z[1][2],\n",
    ")):\n",
    "    ncolfile.write('{} {} {}\\n'.format(PASSAGER_FMT.format(*x), PASSAGE_FMT.format(*y), round(r)))\n",
    "ncolfile.close()\n",
    "\n",
    "all_verses = {(x[0][0]+'r', x[0][1], x[0][2]) for x in crossrefs} | {x[1] for x in crossrefs}\n",
    "all_chapters = {(x[0], x[1]) for x in all_verses}\n",
    "all_books = {x[0] for x in all_chapters}\n",
    "info('{} edges, {} verses, {} chapters, {} books'.format(\n",
    "    len(crossrefs), len(all_verses), len(all_chapters), len(all_books),\n",
    "))\n",
    "print(' '.join(sorted(all_books)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Graph visualization\n",
    "\n",
    "The similarities are visualised in a graph using networkx. The layout is done manually, not following any of the methods provided by networkx.\n",
    "The verses are put into columns by the book they occur in, and the focus chapters occupy a separate column, thanks to the pseudo book Psalms. Psalmsr stands for the other chapters of Psalms. The rows of verses are ordered textually.\n",
    "Finally, the rows of verses are shifted up and down in order to align them with their parallel stretches. The thickness of the edges correponds to the degree of similarity, and likewise, the blacker the edge, the more similar the pair of verses.\n",
    "Then the graph data file is read, the layout settings are adjusted, plotted and saved as pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the graph data\n",
    "g = nx.read_weighted_edgelist(NCOL_FILE)\n",
    "\n",
    "# order the books for handy layout in columns\n",
    "all_books_cust = '''\n",
    "    Job 1_Kings 1_Samuel 2_Chronicles 2_Kings Proverbs Amos Daniel Deuteronomy Ecclesiastes Exodus \n",
    "    Ezekiel Hosea Genesis Habakkuk Ezra 1_Chronicles Psalmsr Numbers Joshua Isaiah Joel Jeremiah Judges Lamentations \n",
    "    Leviticus Malachi Micah Nahum Nehemiah Zechariah 2_Samuel Song_of_songs Psalms\n",
    "'''.strip().split()\n",
    "\n",
    "# Colors used for the graph\n",
    "\n",
    "#b: blue\n",
    "#g: green\n",
    "#r: red\n",
    "#c: cyan\n",
    "#m: magenta\n",
    "#y: yellow\n",
    "#k: black\n",
    "\n",
    "\n",
    "gcolors = {'1_Chronicles':'b', '1_Kings':'g', '1_Samuel':'r', '2_Chronicles':'c', '2_Kings':'m', '2_Samuel':'y', \n",
    "            'Amos':'k', 'Daniel':'b', 'Deuteronomy':'g','Ecclesiastes':'r', 'Exodus':'c', 'Ezekiel':'m', \n",
    "            'Ezra':'y', 'Genesis':'k', 'Habakkuk':'b', 'Hosea':'g', 'Isaiah':'r', 'Psalmsr':'c', 'Psalms':'m', \n",
    "            'Jeremiah':'y', 'Job':'k', 'Joel':'b', 'Joshua':'g', 'Judges':'r', 'Lamentations':'c', 'Leviticus':'m', \n",
    "            'Malachi':'y', 'Micah':'k', 'Nahum':'b', 'Nehemiah':'g', 'Numbers':'r', 'Proverbs':'c', 'Song_of_songs':'c', \n",
    "            'Zechariah':'k'}\n",
    "\n",
    "#  specify vertical positions of passages\n",
    "offset_y = {'1_Chronicles': 1, '1_Kings': 30, '1_Samuel': 20, '2_Chronicles': 12, '2_Kings': 3, '2_Samuel': 15, \n",
    "            'Amos': 30, 'Daniel': 40, 'Deuteronomy': 20,'Ecclesiastes': 3, 'Exodus': 50, 'Ezekiel': 140, \n",
    "            'Ezra': 1, 'Genesis': 140, 'Habakkuk': 17, 'Hosea': 10, 'Isaiah': 120, 'Psalmsr': 60, 'Psalms': 1, \n",
    "            'Jeremiah': 5, 'Job': 40, 'Joel': 25, 'Joshua': 1, 'Judges': 18, 'Lamentations': 120, 'Leviticus': 5, \n",
    "            'Malachi': 145, 'Micah': 78, 'Nahum': 5, 'Nehemiah': 6, 'Numbers': 1, 'Proverbs': 110, 'Song_of_songs': 1, \n",
    "            'Zechariah': 150}\n",
    "\n",
    "# compute positions of verses\n",
    "ncolors = [gcolors[x.split('~')[0]] for x in g.nodes()]\n",
    "nlabels = dict((x, x.split('~')[1]) for x in g.nodes())\n",
    "ncols = len(all_books)\n",
    "pos_x = dict((x, i) for (i,x) in enumerate(all_books_cust))\n",
    "verse_lists = collections.defaultdict(lambda: [])\n",
    "for (bk, ch, vs) in sorted(all_verses):\n",
    "    verse_lists[bk].append('{}:{}'.format(ch, vs))\n",
    "nrows = max(len(verse_lists[bk]) for bk in all_books_cust)\n",
    "pos = {}\n",
    "for bk in verse_lists:\n",
    "    for (i, chvs) in enumerate(verse_lists[bk]):\n",
    "        pos['{}~{}'.format(bk, chvs)] = (pos_x[bk], i+offset_y[bk])\n",
    "\n",
    "# start plotting\n",
    "plt.figure(figsize=(50,30))\n",
    "\n",
    "nx.draw_networkx(g, pos,\n",
    "    width=[g.get_edge_data(*x)['weight']/40 for x in g.edges()],\n",
    "    edge_color=[g.get_edge_data(*x)['weight'] for x in g.edges()],\n",
    "    edge_cmap=plt.cm.Greys,\n",
    "    edge_vmin=50,\n",
    "    edge_vmax=100,\n",
    "    node_color=ncolors,\n",
    "    node_size=200,\n",
    "    labels=nlabels,\n",
    "    alpha=0.4,\n",
    "    linewidths=0,\n",
    ")\n",
    "plt.ylim(-2, 160) #-2, 310\n",
    "book_font_size = 12 # 25\n",
    "plt.grid(b=True, which='both', axis='x')\n",
    "plt.title('Parallel Patterns of Participant Shifts in Psalm 89', fontsize=40)\n",
    "plt.text(-1,120, '''\n",
    "Parallel patterns of participant shifts \n",
    "(i.e. shifts in person, number and gender) \n",
    "in all verses of Ps 89 compared \n",
    "to other books in the Hebrew Bible.\n",
    "\n",
    "Parallel verses shown in this graph have \n",
    "a similarity of 75% or higher. \n",
    "\n",
    "''', #bbox=dict(width=145, height=200, facecolor='yellow', alpha=0.4), fontsize=12)\n",
    "     # suddenly the width and height keyword args are no longer accepted.\n",
    "     # bbox performs an auto fit # plt.text(-1,26, \n",
    "     bbox=dict(facecolor='yellow', alpha=0.4), fontsize=15)\n",
    "\n",
    "# add additional book labels\n",
    "for (ypos, books) in (\n",
    "    (-1, all_books_cust),\n",
    "    (50, ['1_Chronicles']),\n",
    "    (25, ['1_Kings']),\n",
    "    (18, ['1_Samuel']),\n",
    "    (9, ['2_Chronicles']),\n",
    "    (8, ['2_Kings']),\n",
    "    (12, ['2_Samuel']),\n",
    "    (25, ['Amos']),\n",
    "    (37, ['Daniel']),\n",
    "    (17, ['Deuteronomy']),\n",
    "    (10, ['Ecclesiastes']),\n",
    "    (45, ['Exodus']),\n",
    "    (137, ['Ezekiel']),\n",
    "    (30, ['Ezra']),\n",
    "    (137, ['Genesis']),\n",
    "    (15, ['Habakkuk']),\n",
    "    (18, ['Hosea']),\n",
    "    (135, ['Isaiah']),\n",
    "    (106, ['Psalmsr']),\n",
    "    (158, ['Psalms']),\n",
    "    (10, ['Jeremiah']),\n",
    "    (103, ['Job']),\n",
    "    (27, ['Joel']),\n",
    "    (38, ['Joshua']),\n",
    "    (15, ['Judges']),\n",
    "    (130, ['Lamentations']),\n",
    "    (10, ['Leviticus']),\n",
    "    (141, ['Malachi']),\n",
    "    (81, ['Micah']),\n",
    "    (8, ['Nahum']),\n",
    "    (23, ['Nehemiah']),\n",
    "    (52, ['Numbers']),\n",
    "    (105, ['Proverbs']),\n",
    "    (4, ['Song_of_songs']),\n",
    "    (145, ['Zechariah']),\n",
    "    (158, all_books_cust),\n",
    "):\n",
    "    for bk in books:\n",
    "        plt.text(pos_x[bk], ypos, bk, fontsize=book_font_size, horizontalalignment='center')\n",
    "\n",
    "# save the plot as .pdf\n",
    "plt.savefig('psalms_parallels-ps89.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a dataset with extra information\n",
    "\n",
    "Now we have had a quick glance at the parallel verses with the help of the network visualisation, we create a dataset in which we add some extra information. \n",
    "\n",
    "That extra information consists 1. Of a categorisation of the Hebrew Bible books in three main genres: prose, prophecy and poetry. Since we want to know what the genre is of the parallels that are found within the book or chapter we want to study. 2. The network shows that Psalm 89 has a great amount of parallels within the Psalms. Therefore, as a second piece of information, the Psalm collections to which the individual Psalms belong are added to the dataset. The added information of genre and collection enables future analyses of the genre of the Psalm that is studied. \n",
    "\n",
    "If you already ran cells 2.3-2-4, make sure you save the notebook, resart it and then run the cells from 3. onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Genre dictionary for all books in the Hebrew Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prose = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', '1_Samuel', '2_Samuel', \n",
    "         '1_Kings', '2_Kings', 'Jonah', 'Ruth', 'Esther', 'Daniel', 'Ezra', 'Nehemiah', '1_Chronicles', '2_Chronicles']\n",
    "prophecy = ['Isaiah', 'Jeremiah', 'Ezekiel', 'Hosea', 'Joel', 'Obadiah', 'Micah', 'Zephaniah', 'Haggai', 'Zechariah', \n",
    "            'Malachi', 'Amos', 'Nahum', 'Habakkuk']\n",
    "poetry = ['Song_of_songs','Proverbs','Ecclesiastes', 'Lamentations', 'Psalms', 'Job']\n",
    "genre_dict = {}\n",
    "\n",
    "for genre in [prose, prophecy, poetry]:\n",
    "    for book in genre:\n",
    "        if book in prose:\n",
    "            genre_dict[book] = 'prose'\n",
    "        elif book in prophecy:\n",
    "            genre_dict[book] = 'prophecy'\n",
    "        elif book in poetry:\n",
    "            genre_dict[book] = 'poetry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Organize all 150 Psalms in collections in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "introduction = [1,2]\n",
    "\n",
    "davidic = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
    "           29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 138, 139, 140, 141, 142, 143, 144, 145]\n",
    "\n",
    "elohistic_korahite = [42, 43, 44, 45, 46, 47, 48, 49]\n",
    "\n",
    "elohistic_davidic = [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]\n",
    "\n",
    "elohistic_asaphite = [50, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n",
    "\n",
    "korahite = [84, 85, 87, 88]\n",
    "\n",
    "song_of_ascents = [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134]\n",
    "\n",
    "doxology = [146, 147, 148, 149, 150]\n",
    "\n",
    "unclassified = [86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
    "                110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 135, 136, 137]\n",
    "\n",
    "\n",
    "collection_dict = {}\n",
    "\n",
    "for collection in [introduction, davidic, elohistic_korahite, elohistic_davidic, elohistic_asaphite, korahite, \n",
    "                   song_of_ascents, doxology, unclassified]:\n",
    "    for psalm_number in collection:\n",
    "        if psalm_number in introduction:\n",
    "            collection_dict[psalm_number] = 'introduction'\n",
    "        elif psalm_number in davidic:\n",
    "            collection_dict[psalm_number] = 'davidic'\n",
    "        elif psalm_number in elohistic_korahite:\n",
    "            collection_dict[psalm_number] = 'elohistic_korahite'\n",
    "        elif psalm_number in elohistic_davidic:\n",
    "            collection_dict[psalm_number] = 'elohistic_davidic'\n",
    "        elif psalm_number in elohistic_asaphite:\n",
    "            collection_dict[psalm_number] = 'elohistic_asaphite'\n",
    "        elif psalm_number in korahite:\n",
    "            collection_dict[psalm_number] = 'korahite'\n",
    "        elif psalm_number in song_of_ascents:\n",
    "            collection_dict[psalm_number] = 'song_of_ascents'\n",
    "        elif psalm_number in doxology:\n",
    "            collection_dict[psalm_number] = 'doxology'\n",
    "        elif psalm_number in unclassified:\n",
    "            collection_dict[psalm_number] = 'unclassified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create dataset from matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(CHUNK_GREP, 'rb') as f: chunks = pickle.load(f)\n",
    "with open(MATRIX_GREP_PATTERNS_VERSE, 'rb') as f: grep_dist_patterns = pickle.load(f)\n",
    "\n",
    "def get_verse_ref(chunk):\n",
    "    sec = T.sectionFromNode(chunks[chunk][0], lang=LANG)\n",
    "    vn = T.nodeFromSection(sec, lang=LANG)\n",
    "    return(vn, sec)\n",
    "\n",
    "all_verse_nodes = set()\n",
    "crossrefs_patterns_test = set()\n",
    "all_verse_nodes_2 = []\n",
    "\n",
    "n_internal = 0\n",
    "x = 0\n",
    "crossrefs_patterns = set()\n",
    "for (c1, c2, r, c4, c5) in grep_dist_patterns:\n",
    "    (v1, (bk1, ch1, vs1)) = get_verse_ref(c1)\n",
    "    (v2, (bk2, ch2, vs2)) = get_verse_ref(c2)\n",
    "    if r < SIM_THRESHOLD_GREP: continue\n",
    "    if bk1 in REFBOOKS and ch1 in REFCHAPTERS:\n",
    "        bkx = bk1\n",
    "        chx = ch1\n",
    "        vsx = vs1\n",
    "        bky = bk2\n",
    "        chy = ch2\n",
    "        vsy = vs2\n",
    "        pattern1 = c4\n",
    "        pattern2 = c5\n",
    "    elif bk2 in REFBOOKS and ch2 in REFCHAPTERS:\n",
    "        bkx = bk2\n",
    "        chx = ch2\n",
    "        vsx = vs2\n",
    "        bky = bk1\n",
    "        chy = ch1\n",
    "        vsy = vs1\n",
    "        pattern1 = c5\n",
    "        pattern2 = c4\n",
    "    else:\n",
    "        continue\n",
    "    book_genre = genre_dict[bky]\n",
    "    psalm_collection = ''\n",
    "    if bky == 'Psalms' and chy in collection_dict:\n",
    "        psalm_collection = collection_dict[chy]\n",
    "    else:\n",
    "        psalm_collection = 'other'\n",
    "    crossrefs_patterns.add((bkx, str(chx), str(vsx), bky, str(chy), str(vsy), book_genre, psalm_collection, str(round(r)), pattern1, pattern2))\n",
    "    crossrefs_patterns_test.add(((bkx, chx, vsx), (bky, chy, vsy), book_genre, str(round(r)), c4, c5))\n",
    "    all_verse_nodes |= {v1, v2}\n",
    "    all_verse_nodes_2.append((v1, v2))\n",
    "\n",
    "info('{} external crossrefs saved; from total {} crossrefs'.format(\n",
    "    len(crossrefs_patterns), len(grep_dist_patterns),\n",
    "))\n",
    "\n",
    "# quick glance at the data\n",
    "for i in sorted(crossrefs_patterns)[:20]:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Write CSV to disk\n",
    "\n",
    "The CSV file is analysed with Pandas in the levenshtein_png_shifts_visualisation_part3 notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATTERNS_OUTPUT = \"participant-patterns-ratio-s75-verses-psalm-89-with-collection.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info('Writing csv file')\n",
    "\n",
    "with open(PATTERNS_OUTPUT, 'w') as f:\n",
    "    header = ['book', 'chapter', 'verse', 'parallel_book', 'parallel_chapter', 'parallel_verse', 'book_genre_parallel', \n",
    "              'psalm_collection', 'ratio_pattern', 'pattern_psalm', 'pattern_parallel']\n",
    "    f.write('{}\\n'.format(','.join(header)))\n",
    "\n",
    "    for x in sorted(crossrefs_patterns): \n",
    "        f.write('{}\\n'.format(','.join((x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10]))))\n",
    "        \n",
    "info('Done writing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
