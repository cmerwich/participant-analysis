{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum gevonden voor w = [3.200000368025025, 1.6999995384816942]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Dit is een functie waarvan ik weet dat ze een maximum heeft voor\n",
    "# w = [3.2, 1.7], eens kijken of we dat kunnen vinden.\n",
    "def f(w):\n",
    "    return exp(-(3.2 - w[0])**2 - (1.7 - w[1])**2)\n",
    "\n",
    "# Omdat de methode naar een minimum zoekt en wij naar een maximum,\n",
    "# laat ik naar het minimum van het opposite van f() zoeken.\n",
    "\n",
    "def opposite(w):\n",
    "    return -f(w)\n",
    "\n",
    "start_vector = [2, 3] #[2, 3]\n",
    "\n",
    "r = minimize(opposite, start_vector)\n",
    "\n",
    "if r.success:\n",
    "    print(f'Optimum gevonden voor w = [{r.x[0]}, {r.x[1]}]')\n",
    "else:\n",
    "    print('Geen optimum gevonden')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'erwich/sikkel'\n",
    "\n",
    "import os\n",
    "\n",
    "from sys import argv, stderr\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def error(*args, **kwargs):\n",
    "    '''\n",
    "    Prints error messages.\n",
    "    '''\n",
    "\n",
    "    print(*args, file=stderr, **kwargs)\n",
    "    exit(1)\n",
    "\n",
    "def parse_ann(annFile):\n",
    "    '''\n",
    "    Parses a plain text brat annotation file for a given repository\n",
    "    specified in `compare_ann()`, and returns an np.array of corefs and singletons\n",
    "    '''\n",
    "\n",
    "    errors = 0\n",
    "\n",
    "    results_list = []\n",
    "    t2mDict = {}\n",
    "    singletonSet = set()\n",
    "    dataPartsList = []\n",
    "\n",
    "    firstChars = {'T', '#', '*'}\n",
    "    cClass = 0\n",
    "\n",
    "    with open(annFile) as fh:\n",
    "        for (i, line) in enumerate(fh):\n",
    "            epos = f'{i + 1} '\n",
    "            line = line.rstrip('\\n')\n",
    "            firstChar = line[0]\n",
    "\n",
    "            if firstChar not in firstChars:\n",
    "                error(f'{epos}Unrecognized line \"{line}\"')\n",
    "                errors +=1\n",
    "                continue\n",
    "\n",
    "            numFields = 2 if firstChar =='*' else 3\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            if len(parts) != numFields:\n",
    "                error(f'{epos}line does not have exactly {numFields} parts: \"{line}\"')\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            if firstChar == 'T':\n",
    "                (tPart, mentionStr, aWord) = parts\n",
    "                mParts = mentionStr.split()\n",
    "                if len(mParts) != 3:\n",
    "                    error(f'{epos}T-line mention does not have exactly 3 parts: \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                t2mDict[tPart] = mentionStr\n",
    "                singletonSet.add(mentionStr)\n",
    "\n",
    "            elif firstChar == '*':\n",
    "                corefSets = set()\n",
    "                (char, data) = parts\n",
    "                dataParts = data.split()\n",
    "                if len(dataParts) <= 1 or dataParts[0] != 'Coreference':\n",
    "                    error(f'{epos}*-line spec does not have the right parts: \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                cClass += 1\n",
    "                dataPartsList.append(dataParts)\n",
    "        \n",
    "        for l in dataPartsList:\n",
    "            corefSets = set()\n",
    "            for tPart in l[1:]:\n",
    "                if tPart in corefSets:\n",
    "                    error(f'{epos}*-\"{tPart} occurs in multiple classes \"{corefSets[tPart]}\" in \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                corefSets.add(t2mDict[tPart])\n",
    "                singletonSet.discard(t2mDict[tPart])\n",
    "            \n",
    "            results_list.append(corefSets)\n",
    "        \n",
    "        results_list.append(singletonSet) #results_list[-1] is the singletonSet\n",
    "    \n",
    "    if errors:\n",
    "        error(f'There are {errors} errors in annotation file')\n",
    "    \n",
    "    results_array = np.array(results_list)\n",
    "\n",
    "    return results_array\n",
    "\n",
    "def selection_size(L, s):\n",
    "    '''\n",
    "    L is a list of sets, s is a set of indices in L. This function\n",
    "    returns the total of the cardinalities of the sets selected by s.\n",
    "    '''\n",
    "    \n",
    "    r = 0\n",
    "    for e in s:\n",
    "        r += len(L[e])\n",
    "    return r\n",
    "\n",
    "def distance(s1, s2):\n",
    "    return (len(s1 - s2), len(s1 & s2), len(s2 - s1))\n",
    "    \n",
    "def match(A, B, d):\n",
    "    '''\n",
    "    Matches the nodes in a bipartite graph with n and k nodes using the\n",
    "    distance function d(i,j) and stores the matching in array r.\n",
    "    The unpaired corefs are calculated by selection_size().\n",
    "    '''\n",
    "\n",
    "    n = len(A)\n",
    "    k = len(B)\n",
    "    cost = np.zeros((n, k))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(k):\n",
    "            (L, M, R) = distance(A[i], B[j])\n",
    "            cost[i, j] = (L + R) / (L + M + R)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "    unpaired_A = selection_size(A, set(range(n)) - set(row_ind))\n",
    "    unpaired_B = selection_size(B, set(range(k)) - set(col_ind))\n",
    "    total_cost = cost[row_ind, col_ind].sum() + unpaired_A + unpaired_B\n",
    "    uA = set(range(n)) - set(row_ind)\n",
    "    uB = set(range(k)) - set(col_ind)\n",
    "    \n",
    "    return total_cost.sum(), row_ind, col_ind, uA, uB\n",
    "\n",
    "def make_r(B):\n",
    "    '''\n",
    "    Makes an array of zero's with the length of array B.\n",
    "    '''\n",
    "    \n",
    "    r = np.zeros(len(B), dtype=int)\n",
    "    return r\n",
    "\n",
    "def set_distance(sA, sB):\n",
    "    '''\n",
    "    Prints the paired and unpaired coref sets of annotator A and B.\n",
    "    Returns the difference (Left, Right),\n",
    "    intersection (M), symmetric difference (D) and distance() (d) of two sets. \n",
    "    '''\n",
    "    \n",
    "    L = len(sA-sB)\n",
    "    M = len(sA&sB)\n",
    "    R = len(sB-sA)\n",
    "    #D = L + R\n",
    "    #d = D/(L+M+R)\n",
    "    sum_it(L, M, R)\n",
    "    return L, M, R\n",
    "\n",
    "def sum_it(L, M, R):\n",
    "    '''\n",
    "    Takes as input a txt file, tab separated.\n",
    "    Accumulates the coref differences per txt file\n",
    "    and prints its total on standard output.\n",
    "    Example call: python3 acc.py Psalms_*.ann\n",
    "    '''\n",
    "        \n",
    "    Lt += L\n",
    "    Mt += M\n",
    "    Rt += R\n",
    "    return Lt, Mt, Rt\n",
    "\n",
    "def compare_corefs(A, B, uA, rx, cx, uB):\n",
    "    for i in range(len(rx)):\n",
    "        l, m, r = set_distance(A[rx[i]], B[cx[i]])\n",
    "    l1, m1, r1 = sum_it(l, m, r)\n",
    "    for i in uA:\n",
    "        L, M, R = set_distance(A[i], set())\n",
    "    l2, m2, r2 = sum_it(l, m, r)\n",
    "    for i in uB:\n",
    "        L, M, R = set_distance(set(), B[i]) \n",
    "    l3, m3, r3 = sum_it(l, m, r)\n",
    "    \n",
    "    D = l1 + l2 + l3 + r1 + r2 + r3\n",
    "    d = D/(l1 + m1 + r1 + l2 + m2 + r2 + l3 + m3 + r3)\n",
    "    return d\n",
    "\n",
    "def CompareAnn(pathA, pathB):\n",
    "    '''\n",
    "    Executes the comparison between the brat annotation files of annotator A and B.\n",
    "    '''\n",
    "    \n",
    "    annFileA = os.path.expanduser(pathA)\n",
    "    annFileB = os.path.expanduser(pathB)\n",
    "    \n",
    "    results_array_A = parse_ann(annFileA)\n",
    "    results_array_B = parse_ann(annFileB)\n",
    "    \n",
    "    make_r(results_array_B)\n",
    "    cost, rx, cx, uA, uB = match(results_array_A, results_array_B, distance)\n",
    "    d = compare_corefs(results_array_A, results_array_B, uA, rx, cx, uB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    CompareAnn(argv[1], argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Christiaan/github/cmerwich/participant-analysis/mimi/mimi-opt\n"
     ]
    }
   ],
   "source": [
    "new_ann = os.path.expanduser('~/github/cmerwich/participant-analysis/mimi/mimi-opt/*.ann')\n",
    "out = '*.ann'\n",
    "old_file = os.path.expanduser('~/Sites/brat/data/coref/Psalms/annotate/*.txt') # old file .txt\n",
    "mimi_file = os.path.expanduser('~/github/cmerwich/participant-analysis/mimi/mimi-opt/*.txt') # mimi file .txt\n",
    "\n",
    "path_a = '~/Sites/brat/data/coref/Psalms/annotate'\n",
    "path_b = '~/github/cmerwich/participant-analysis/mimi/mimi-opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import exp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from functools import lru_cache\n",
    "from translate import translate\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def Optimise(w):\n",
    "    '''\n",
    "    w = weight_vector = []\n",
    "    '''\n",
    "    \n",
    "    mimi('Psalms', 105, 105, w)\n",
    "    new_ann = os.path.expanduser('~/github/cmerwich/participant-analysis/mimi/mimi-opt/Psalms_105.ann') # mimi file.ann\n",
    "    out = 'Psalms_105_n.ann'\n",
    "    old_file = os.path.expanduser('~/Sites/brat/data/coref/Psalms/annotate/Psalms_105.txt') # old file .txt\n",
    "    mimi_file = os.path.expanduser('~/github/cmerwich/participant-analysis/mimi/mimi-opt/Psalms_105.txt') # mimi file .txt\n",
    "    text_files = []\n",
    "    text_files.append(old_file)\n",
    "    text_files.append(mimi_file)\n",
    "    translate(new_ann, out, text_files)\n",
    "    path_a = os.path.expanduser('~/Sites/brat/data/coref/Psalms/annotate/Psalms_105.ann')\n",
    "    d = CompareAnn(path_a, new_ann) #CompareAnn(path_a, path_b)\n",
    "    r = minimize(d, w)\n",
    "    \n",
    "    if r.success:\n",
    "        print(f'Max found for w = [{r.x[0]}, {r.x[1]}, {r.x[2]}]')\n",
    "    else:\n",
    "        print('No max found')\n",
    "Optimise([[0.1, 3.0, 3.0]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stanford:\n",
    "Sieve 1: Wanneer 1e en 2e persoon coreferen \n",
    "Sieve 2: Exact head string match sieve\n",
    "Entity sieve\n",
    "3P pronoun aan het einde van MiMi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exact head string match sieve\n",
    "\n",
    "Alle sets aflopen corefs en singletons \n",
    "Eerst het lexeem, exacte woord \n",
    "\n",
    "Als je strikte match gebruikt, vind je te weinig matches; \n",
    "als je alleen head gebruikt (woord in cst) dan vind je te veel. \n",
    "Dus strategie is: los met aantal restricties. \n",
    "\n",
    "doorzoek alle singletons \n",
    "Als je een singleton hebt, dan teruglopen in de tekst, match, dan unite \n",
    "\n",
    "mentions die al in een coref zitten, fungeren als potentiÃ«le match \n",
    "\n",
    "Je loopt vanaf een singleton en corefs terug totdat de eigen tekst positie bereikt (node_tuple gebruiken)\n",
    "\n",
    "= Als er meerdere matches zijn, de mention matchen die het dichtst bij staat. \n",
    "Afstand moet klein zijn. Aanname is hoe dichter de te mentions bij elkaar staan, hoe waarschijnlijker het is \n",
    "dat ze naar hetzelfde verwijzen. \n",
    "= Afstand meenemen \n",
    "= mate van overeenkomst in string \n",
    "\n",
    "Doorzoek alle sets van mentions:\n",
    "Gebruik dan singletons, doorzoek net zo lang totdat de doorzochte mentions qua node niet meer kleiner dan de zoek-mention \n",
    "\n",
    "NMPR uitsluiten \n",
    "for m in singletons: \n",
    "    for coref in coref_lists and singletons: \n",
    "        for element in corefs \n",
    "        i = 0 \n",
    "        while i < len(coref) and coref[i] < m.tekst_positie:\n",
    "            1. Doe match op basis van een aantal criteria:  \n",
    "            vergelijking doen:\n",
    "            Tekst afstand meenemen (verschil in node nummer)\n",
    "            mate van overeenkomst in string\n",
    "                - getal/geslacht\n",
    "                - hoofd van de mention, in cst \n",
    "                verschillende features combineren om score te berekenen. \n",
    "            \n",
    "            2. Score berekenen voor rang in queu: (score functie)\n",
    "            bij elke match een score uitrekenen, die score bepaalt de positie in de wachtrij. \n",
    "            aan het einde van de zoektocht is er een volle wachtrij, de beste hit wordt vooraan gezet, \n",
    "            gebruik dqueu om eerste uit wachtrij te halen\n",
    "                priority queu, data structuur. Normaal(nqueu (aansluiten), dqueu (bedienen))\n",
    "            - Als ik klaar ben met het doorzoeken van de singletons, en alle mentions in de wachtrij staan, \n",
    "            dan dqueu\n",
    "            - Dan unite\n",
    "            - Maak queu leeg\n",
    "            \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity sieve\n",
    "\n",
    "zelfde procedure als voor de exact string match, \n",
    "behalve score functie en de criteria voor de match zijn anders\n",
    "voorsortering \n",
    "\n",
    "twee functies:\n",
    "    matchfunctie\n",
    "    waarderingsfunctuie\n",
    "     \n",
    "for m in mentions:\n",
    "    if m.rpt == NMPR:\n",
    "        1. eerst matchen op bep. criteria \n",
    "        - bijv. op hoofd\n",
    "        - Orthogonale features (features die elkaar niet beinvloeden gebruiken voor vector), \n",
    "        vector maken: elke feature is een component van de vector \n",
    "        - vector maken van persoon, geslacht, getal. Lengte van vector is mate van overeenkomst score. \n",
    "        - Euclidische afstand van vector wordt score. In geval van overeenkomst 0 toekenen, \n",
    "        in geval van niet overeenkomst niet 0 toekennen. Elke feature heeft ook een gewicht. \n",
    "        Uitzoeken welk gewicht ik per feature toeken\n",
    "        2, 3, 5\n",
    "        Euclidische afstand = wortel(sum(elk element uit de vector in het kwadraat))\n",
    "        lengte = wortel(38)\n",
    "        2. Score berekenen op basis van die criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aanpak:\n",
    "    - eerst kader van beide zeven opzetten in pseudo code\n",
    "    - eerst werkend krijgen van matchfunctie, waarderingsfunctie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from heapq import heappush, heappop\n",
    "import math\n",
    "\n",
    "def heapsort(iterable):\n",
    "    h = []\n",
    "    for value in iterable:\n",
    "         heappush(h, value)\n",
    "    \n",
    "    return [heappop(h) for i in range(len(h))]\n",
    "heapsort([1, 3, 5, 7, 9, 2, 4, 6, 8, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.164414002968976\n"
     ]
    }
   ],
   "source": [
    "l = [2, 3, 5]\n",
    "i_sum = 0\n",
    "euc = 0\n",
    "for i in l:\n",
    "    i_sqrt = i**2\n",
    "    i_sum += i_sqrt\n",
    "    euc = math.sqrt(i_sum)\n",
    "print(euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.164414002968976"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.15, 3, 'write spec', 'm')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = []\n",
    "heappush(h, (5.22, 4, 'write code', 'm'))\n",
    "heappush(h, (7.23, 5, 'release product', 'm'))\n",
    "heappush(h, (1.15, 3, 'write spec', 'm'))\n",
    "heappush(h, (1.99, 2, 'create tests', 'm'))\n",
    "n = heappop(h)\n",
    "n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ps 105\n",
    "\n",
    "Regel: als beide mentions in dezelfde phrase zitten, dus zelfde hoofd hebben, uitsluiten\n",
    "\n",
    "# zelfde phrase, fout\n",
    "united strings ('MCPVJ PJ', (328620, 328621)) ('MPTJ', (328618, 328618)) \n",
    "\n",
    "# klopt niet, zelfde head, verwijst niet naar elkaar\n",
    "united strings ('KL H->RY', (328632, 328634)) ('KL NPL>WTJ', (328597, 328598)) \n",
    "\n",
    "# zelfde phrase fout\n",
    "united strings ('M<V', (328677, 328677)) ('MTJ MSPR', (328674, 328675)) \n",
    "\n",
    "# niet in deze zelfde phrase, maar hebben voor alle criteria een match\n",
    "# united strings ('GWJ', (328686, 328686)) ('GWJ', (328684, 328684)) \n",
    "\n",
    "# klopt niet, zelfde head, verwijst niet naar elkaar, lexeem van KL gebruiken, matchen op tweede woord in phrase met nme als die er is, anders KL gebruiken\n",
    "# regel voor lexical set, KL, ls=nmdi, dan ook matchen op tweede woord\n",
    "united strings ('KL MVH LXM', (328716, 328718)) ('KL H->RY', (328632, 328634)) \n",
    "\n",
    "united strings ('KL MVH LXM', (328716, 328718)) ('KL NPL>WTJ', (328597, 328598)) \n",
    "\n",
    "united strings ('KL MVH LXM', (328716, 328718)) ('KL H->RY', (328632, 328634)) \n",
    "\n",
    "united strings ('KL MVH LXM', (328716, 328718)) ('KL NPL>WTJ', (328597, 328598)) \n",
    "\n",
    "# klopt niet, PNJ hebben allebei ook ander suffix \n",
    "united strings ('PNJ', (328722, 328722)) ('PNJ', (328612, 328612)) \n",
    "\n",
    "factor 5\n",
    "0.2772588722239781 2\n",
    "0.13862943611198905\n",
    "\n",
    "1.3735948817940586 31\n",
    "0.6867974408970293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with (2.0, 3.0)\n",
      "Called with (2.000000014901161, 3.0)\n",
      "Called with (2.0, 3.000000014901161)\n",
      "Called with (2.1049227146431804, 2.886333729606122)\n",
      "Called with (2.1049227295443416, 2.886333729606122)\n",
      "Called with (2.1049227146431804, 2.886333744507283)\n",
      "Called with (2.524613573215902, 2.431668648030609)\n",
      "Called with (2.524613588117063, 2.431668648030609)\n",
      "Called with (2.524613573215902, 2.43166866293177)\n",
      "Called with (4.203377007506788, 0.6130083217285573)\n",
      "Called with (4.203377022407949, 0.6130083217285573)\n",
      "Called with (4.203377007506788, 0.6130083366297185)\n",
      "Called with (3.055938426554678, 1.8560667428380198)\n",
      "Called with (3.0559384414558393, 1.8560667428380198)\n",
      "Called with (3.055938426554678, 1.856066757739181)\n",
      "Called with (3.187728733213546, 1.7132939154009559)\n",
      "Called with (3.1877287481147074, 1.7132939154009559)\n",
      "Called with (3.187728733213546, 1.713293930302117)\n",
      "Called with (3.5502245639218533, 1.3205899608587774)\n",
      "Called with (3.5502245788230145, 1.3205899608587774)\n",
      "Called with (3.5502245639218533, 1.3205899757599386)\n",
      "Called with (3.1988848074289686, 1.7012081640889976)\n",
      "Called with (3.19888482233013, 1.7012081640889976)\n",
      "Called with (3.1988848074289686, 1.7012081789901587)\n",
      "Called with (3.200000368025025, 1.6999995384816942)\n",
      "Called with (3.200000382926186, 1.6999995384816942)\n",
      "Called with (3.200000368025025, 1.6999995533828554)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.000000\n",
      "         Iterations: 3\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 9\n",
      "Optimum gevonden voor w = [3.200000368025025, 1.6999995384816942]\n",
      "CacheInfo(hits=9, misses=27, maxsize=32, currsize=27)\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "from math import exp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Dit is een functie waarvan ik weet dat ze een maximum heeft voor\n",
    "# w = [3.2, 1.7], eens kijken of we dat kunnen vinden.\n",
    "@lru_cache(maxsize=32)\n",
    "def f(w):\n",
    "   print('Called with', w)\n",
    "   return exp(-(3.2 - w[0])**2 - (1.7 - w[1])**2)\n",
    "\n",
    "# Omdat de methode naar een minimum zoekt en wij naar een maximum,\n",
    "# laat ik naar het minimum van het tegengestelde van f() zoeken.\n",
    "\n",
    "def tegengestelde(w, book, c1, c2):\n",
    "   return -f(tuple(w))\n",
    "\n",
    "start_vector = [2, 3]\n",
    "\n",
    "r = minimize(tegengestelde, start_vector, args = ('Psalms', 23, 34),\n",
    "\t     options={'disp': True})\n",
    "\n",
    "if r.success:\n",
    "   print(f'Optimum gevonden voor w = [{r.x[0]}, {r.x[1]}]')\n",
    "else:\n",
    "   print('Geen optimum gevonden')\n",
    "\n",
    "print(f.cache_info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.01)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ (1, 7.57), (2, 2.1), (3, 1.2), (4, 2.1), (5, 0.01), \n",
    "         (6, 0.5), (7, 0.2), (8, 0.6)]\n",
    "min(data, key = lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_chris(w, book, from_chapter, to_chapter):\n",
    "\n",
    "    '''\n",
    "    w = weight_vector = []\n",
    "    '''\n",
    "    w_list = []\n",
    "    r = np.array([0, 0, 0])\n",
    "    mimi(book, from_chapter, to_chapter, w)\n",
    "    for c in range(from_chapter, to_chapter +1):\n",
    "        PATH_A, PATH_B = do_translate(book, c)\n",
    "        #ps = f'{book}_{c:>03}'\n",
    "        #mimi_ann = f'{ps}.ann'\n",
    "        #backup_ann_w = f'{ps}_{w[0]}_{w[1]}_{w[2]}.ann'\n",
    "        #os.system(f'cp {mimi_ann} {backup_ann_w}')\n",
    "        \n",
    "        #print(subprocess.check_output(['md5', f'{PATH_B}']))\n",
    "        r += CompareAnn(PATH_A, PATH_B)\n",
    "    print((r[0] + r[2]) / sum(r), 'w: ', w)\n",
    "    #w_list.append(((r[0] + r[2]) / sum(r), w))\n",
    "    #get_min(w_list)\n",
    "    return (r[0] + r[2]) / sum(r) #distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
