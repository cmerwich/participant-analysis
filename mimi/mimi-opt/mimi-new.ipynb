{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'erwich/sikkel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getopt\n",
    "import os, sys\n",
    "from sys import argv, exit, stderr\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from operator import attrgetter\n",
    "from functools import lru_cache #\n",
    "import re\n",
    "from shutil import rmtree\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from tf.app import use\n",
    "from sly import Lexer, Parser\n",
    "\n",
    "from scipy.optimize import minimize, linear_sum_assignment\n",
    "from translate import translate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB = '\\t'\n",
    "NL = '\\n'\n",
    "VERSION = 'c'\n",
    "TYP_CHOICE = {'NP', 'PP', 'PrNP', 'PPrP', 'VP', 'DPrP', 'IPrP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s Dataset without structure sections in otext:no structure functions in the T-API\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf, td.tf, th.tf { text-align: left ! important;}@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: large;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: x-large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: x-large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "    text-align: start;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 0.1rem;\n",
       "    margin: 0.1rem;\n",
       "    direction: ltr;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1rem 0rem;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".section {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--section);\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.row {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.row.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.col {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.col.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  0.5rem 0.1rem 0.1rem 0.1rem;\n",
       "    margin: 0.8rem 0.1rem 0.1rem 0.1rem;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       ".contnr.l {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".contnr.r {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".contnr.lno {\n",
       "    border-left-style: none\n",
       "}\n",
       ".contnr.rno {\n",
       "    border-right-style: none\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -1.2rem;\n",
       "    margin-left: 1rem;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3rem;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 0.1rem;\n",
       "    margin-left: 0.1rem;\n",
       "    padding: 0.1rem 0.1rem;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:             hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:            hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--section:          hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:        hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:         hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:       hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:        hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:       hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:       0.15rem;\n",
       "  --border-color-nul: hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:    hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:    hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:    hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:    hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:    hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul: 0.1rem;\n",
       "  --border-width0:    0.1rem;\n",
       "  --border-width1:    0.15rem;\n",
       "  --border-width2:    0.2rem;\n",
       "  --border-width3:    0.3rem;\n",
       "  --border-width4:    0.25rem;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 0.2rem ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 0.2rem;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.3rem;\n",
       "  padding: 0.2rem;\n",
       "  margin: 0.2rem;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def error(*args, **kwargs):\n",
    "    print(*args, file=stderr, **kwargs)\n",
    "    exit(1)\n",
    "    \n",
    "def Usage():\n",
    "    stderr.write('usage: mimi.py book_name first_chapter [last_chapter]\\n')\n",
    "    exit(1)\n",
    "    \n",
    "A = use('bhsa:local', \n",
    "        checkout='local',\n",
    "        version = VERSION,\n",
    "        hoist=globals(),\n",
    "        silent=True\n",
    "       )\n",
    "TF.load('g_prs', add=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mention:\n",
    "    def __init__(self, text='', start=0, end=0, node_tuple=tuple(), \n",
    "                 person='', gender='', number='', issuffix = False, rpt ='', function=''):\n",
    "        \n",
    "        self.text = text               # Lexical information of the mention\n",
    "        self.start = start             # Start of the word position in the txt file\n",
    "        self.end = end                 # End of the position in the txt file\n",
    "        self.node_tuple = node_tuple   # Text-Fabric nodes of the mention(s)\n",
    "        self.person = person           # Grammatic person of word object \n",
    "        self.gender = gender           # Grammatic gender of word object \n",
    "        self.number = number           # Grammatic number of word object \n",
    "        self.issuffix = issuffix       # Boolean for existence of suffix on word\n",
    "        self.rpt = rpt                 # Reconstructed phrase type\n",
    "        self.function = function       # Function of mention(s) within phrase object (e.g. object, subject)\n",
    "        self.pargr = ''                # Paragraph number of the mention within the clause_atom object\n",
    "        self.txttype = ''              # Text type of the mention within the clause object: ?, N, D, Q\n",
    "        self.name = ''                 # Identifier of the mention, e.g. T32\n",
    "        self.note = ''                 # AnnotatorNotes generated by MakeMentions()\n",
    "        self.file = ''                 # File object identifier  \n",
    "        self.corefclass = set()        # Coreference class to which the mention belongs\n",
    "        self.who = ''                  # Identification of a class as entity\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        #return self.name + ' ' + self.text\n",
    "        return self.text\n",
    "    \n",
    "    def __gt__(self, other): \n",
    "        return self.node_tuple[0] > other.node_tuple[0]\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.node_tuple[0] < other.node_tuple[0]\n",
    "        \n",
    "class Token:\n",
    "    '''\n",
    "    Token object for Sly's lexer. \n",
    "    - type: the Token type\n",
    "    - mention: Mention object\n",
    "    - node: node number of the word. \n",
    "    For suffixes the node of the word it was attached to is used. \n",
    "    '''\n",
    "    def __init__(self, type='', mention='', node=0):\n",
    "        self.type = type\n",
    "        self.value = mention\n",
    "        self.node = node\n",
    "        \n",
    "class Statistics:\n",
    "    '''\n",
    "    Contains descriptive statistics for the two phases of MiMi:\n",
    "    1. Mention detection: `pa_count' (phrase atom count), `error_count', and `rule_count'\n",
    "    2. Coreference resolution: `input_corefs' and `out_corefs'. input_corefs is a list of singleton sets.\n",
    "    output_corefs is a list of unified coreference classes, singletons and unresolved mentions. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Mention detection\n",
    "        self.pa_count = 0 \n",
    "        self.error_count = 0\n",
    "        self.discarded = 0 \n",
    "        self.created = 0\n",
    "        self.rule_count = Counter()\n",
    "        self.fc_parsed = FuzzyCounter()\n",
    "        self.fc_failed = FuzzyCounter()\n",
    "        \n",
    "        # Coreference resolution\n",
    "        self.input_corefs = 0\n",
    "        self.output_corefs = 0\n",
    "        self.coref_classes = 0\n",
    "        \n",
    "        # Sieve counters\n",
    "        self.resolve_predicate = 0\n",
    "        self.resolve_1p_2p_pronouns = 0\n",
    "        self.resolve_vocative = 0\n",
    "        self.resolve_apposition = 0\n",
    "        self.resolve_fronted = 0\n",
    "        # extra\n",
    "        self.match_string = 0\n",
    "        self.resolve_entity = 0\n",
    "        self.resolve_3p_pronouns = 0\n",
    "        \n",
    "    def __iadd__(self, other):\n",
    "        self.pa_count += other.pa_count\n",
    "        self.error_count += other.error_count\n",
    "        self.input_corefs += other.input_corefs\n",
    "        self.output_corefs += other.output_corefs\n",
    "        self.coref_classes += other.coref_classes\n",
    "        self.resolve_predicate += other.resolve_predicate\n",
    "        self.resolve_1p_2p_pronouns += other.resolve_1p_2p_pronouns\n",
    "        self.resolve_vocative += other.resolve_vocative\n",
    "        self.resolve_apposition += other.resolve_apposition\n",
    "        self.resolve_fronted += other.resolve_fronted\n",
    "        self.match_string += other.match_string   # extra\n",
    "        self.resolve_entity += other.resolve_entity           # extra\n",
    "        self.resolve_3p_pronouns += other.resolve_3p_pronouns # extra\n",
    "        self.rule_count += other.rule_count\n",
    "        self.fc_parsed += other.fc_parsed\n",
    "        self.fc_failed += other.fc_failed\n",
    "        return self        \n",
    "        \n",
    "    def mention_success(self):\n",
    "        #return 1 - self.error_count / self.pa_count\n",
    "        return 1 - self.fc_failed.count / self.fc_parsed.count\n",
    "    \n",
    "    def mention_failure(self):\n",
    "        #return self.error_count / self.pa_count\n",
    "        return self.fc_failed.count / self.fc_parsed.count\n",
    "    \n",
    "    def coref_success(self):\n",
    "        return (self.input_corefs - self.output_corefs) / self.input_corefs\n",
    "    \n",
    "    def coref_unresolved(self):\n",
    "        return self.output_corefs / self.input_corefs\n",
    "\n",
    "class FuzzyCounter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.bonus = 0\n",
    "        self.malus = 0\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        self.count += other.count\n",
    "        self.bonus += other.bonus\n",
    "        self.malus += other.malus\n",
    "        return self\n",
    "    \n",
    "    def inc(self):\n",
    "        self.count += 1\n",
    "    \n",
    "    def total(self):\n",
    "        return self.count + self.bonus - self.malus\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.count}+{self.bonus}-{self.malus}'\n",
    "    \n",
    "class MyLexer(Lexer):\n",
    "    '''\n",
    "    From sly. \n",
    "    '''\n",
    "    \n",
    "    tokens = {ADJV_A,\n",
    "             ADJV_C,\n",
    "             ADJV_E,\n",
    "             ADVB,\n",
    "             ART,\n",
    "             CONJ,\n",
    "             CONJ_P,\n",
    "             EOA,\n",
    "             INRG,\n",
    "             INTJ,\n",
    "             NEGA,\n",
    "             NMPR,\n",
    "             PRDE,\n",
    "             PREP,\n",
    "             PRIN,\n",
    "             PRPS,\n",
    "             PRS,\n",
    "             SUBS_A,\n",
    "             SUBS_C,\n",
    "             SUBS_E,\n",
    "             SUBS_P,\n",
    "             VERB \n",
    "             }\n",
    "    \n",
    "    # Define a rule so we can track line numbers\n",
    "    @_(r'\\n+')\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += len(t.value)\n",
    "    \n",
    "    def tokenize(self, my_tokens):\n",
    "        i = 0\n",
    "        while i < len(my_tokens):\n",
    "            yield my_tokens[i]\n",
    "            i += 1\n",
    "    \n",
    "    # Error handling rule\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Token 'ADJV_E' defined, but not used\n",
      "WARNING: Token 'SUBS_E' defined, but not used\n",
      "WARNING: There are 2 unused tokens\n",
      "WARNING: 1 shift/reduce conflict\n",
      "Parser debugging for MyParser written to parser.out\n"
     ]
    }
   ],
   "source": [
    "class MyParser(Parser):\n",
    "    \n",
    "    def __init__(self, statistics):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.Chapter_Mentions = []\n",
    "        self.statistics = statistics\n",
    "        self.created = 0 # Reset per phrase atom. Count extra mention(s) per phrase atom\n",
    "        self.discarded = 0 # Reset per phrase atom. Count phrase atom\n",
    "        self.appended = 0 # Counts per phrase atom each time a mention is appended to Chapter_Mentions list\n",
    "    \n",
    "    debugfile = 'parser.out'\n",
    "    tokens = MyLexer.tokens\n",
    "    \n",
    "    def append(self, mention):\n",
    "        self.Chapter_Mentions.append(mention)\n",
    "        self.appended += 1\n",
    "    \n",
    "    def error(self, p):\n",
    "        '''\n",
    "        Writes syntax errors for tokens that are not parseable \n",
    "        for some reason to a file `mention_errors`. \n",
    "        '''\n",
    " \n",
    "        if p:\n",
    "            mention_errors.write(f'{p.type}, {p.value.text}, start={p.value.start}, node={p.node}\\n')\n",
    "        else:  \n",
    "            mention_errors.write('Syntax error at EOF\\n')\n",
    "            print('Syntax error at EOF')\n",
    "        self.statistics.error_count += 1\n",
    "        \n",
    "    def fuzzycount(self, fc):\n",
    "        fc.inc()\n",
    "        fc.malus += self.discarded\n",
    "        fc.bonus += self.created\n",
    "        self.discarded = 0\n",
    "        self.created = 0\n",
    "        self.appended = 0\n",
    "    \n",
    "    # GRAMMAR RULES FOR BIBLICAL HEBREW PHRASE ATOMS # \n",
    "    \n",
    "    @_('term_atom', 'text term_atom')\n",
    "    def text(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return self.Chapter_Mentions\n",
    "    \n",
    "    @_('phrase_atom EOA')\n",
    "    def term_atom(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.fuzzycount(self.statistics.fc_parsed)\n",
    "        \n",
    "        # debugging \n",
    "        n = self.statistics.rule_count[self.production.number]\n",
    "        e = self.statistics.error_count\n",
    "        l = len(self.Chapter_Mentions)\n",
    "        d = self.statistics.discarded\n",
    "        c = self.statistics.created\n",
    "        \n",
    "        if len(self.Chapter_Mentions) != self.statistics.fc_parsed.total() + self.statistics.fc_failed.total():\n",
    "            print(self.statistics.fc_parsed, self.statistics.fc_failed)\n",
    "            print(n, e, l, d, c, f'Het is misgegaan bij {p[0].node_tuple}, {p[0].text}, {p[0].rpt}')\n",
    "            assert(False)\n",
    "        return p[0]\n",
    "    \n",
    "    @_('error EOA')\n",
    "    def term_atom(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        #print(p[1].text, p[1].start, p[1].end)\n",
    "        if self.appended == 0:\n",
    "            self.discarded = 1\n",
    "        #else:\n",
    "            # print last sucessful mention in self.Chapter_Mentions[-1]\n",
    "        #    print(p[0].text, p[0].node_tuple, p[0].rpt)\n",
    "        self.fuzzycount(self.statistics.fc_failed)\n",
    "    \n",
    "    @_('interrogative_phrase', 'adverbial_phrase', 'complex_prepositional_phrase', \n",
    "       'verbal_phrase', 'subject_suffix')\n",
    "    def phrase_atom(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        #print('Mention', p[0].text, p[0].start, p[0].end)\n",
    "        return p[0]\n",
    "    \n",
    "    @_('complex_noun_phrase', 'complex_adjective_phrase')\n",
    "    def phrase_atom(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.append(p[0])\n",
    "        return p[0]\n",
    "    \n",
    "    @_('normal_VP', 'prepositional_VP')\n",
    "    def verbal_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('PREP verbal_phrase')\n",
    "    def prepositional_VP(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[1]\n",
    "    \n",
    "    @_('VERB')\n",
    "    def normal_VP(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.append(p[0])\n",
    "        return p[0] \n",
    "    \n",
    "    @_('VERB PRS')\n",
    "    def normal_VP(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.append(p[0])\n",
    "        return p[0]\n",
    "    \n",
    "    @_('noun_phrase')\n",
    "    def complex_noun_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('complex_noun_phrase noun_phrase')\n",
    "    def complex_noun_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return CombineMentions(p[0], ' ', p[1])\n",
    "    \n",
    "    @_('adjective_phrase')\n",
    "    def complex_adjective_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('complex_adjective_phrase CONJ adjective_phrase')\n",
    "    def complex_adjective_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        m = CombineMentions(p[0], ' ' + p[1].text, p[2])\n",
    "        m.number = 'pl'\n",
    "        return m\n",
    "    \n",
    "    @_('complex_noun_phrase CONJ noun_phrase')\n",
    "    def complex_noun_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        m = CombineMentions(p[0], ' ' + p[1].text, p[2])\n",
    "        m.number = 'pl'\n",
    "        return m\n",
    "    \n",
    "    @_('complex_prepositional_phrase CONJ_P prepositional_phrase')\n",
    "    def complex_prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.created += 1\n",
    "        #print('created', p[0].text, p[2].text)\n",
    "        m = CombineMentions(p[0], ' ' + p[1].text, p[2])\n",
    "        m.number = 'pl'\n",
    "        return m\n",
    "    \n",
    "    @_('prepositional_phrase')\n",
    "    def complex_prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('focussed_prepositional_phrase')\n",
    "    def prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('ADVB focussed_prepositional_phrase')\n",
    "    def focussed_prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[1]\n",
    "    \n",
    "    @_('extended_prepositional_phrase')\n",
    "    def focussed_prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('PREP extended_prepositional_phrase')\n",
    "    def extended_prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[1]\n",
    "    \n",
    "    @_('PREP complex_noun_phrase')\n",
    "    def extended_prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.append(p[1])\n",
    "        return p[1]\n",
    "        \n",
    "    @_('PREP ADVB', 'PREP PRIN', 'PREP INRG')\n",
    "    def prepositional_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.discarded += 1\n",
    "        #print('discarding', p[0].text, p[1].text)\n",
    "        sep = '' if p[0].text[-1] == '-' else ' '\n",
    "        return CombineMentions(p[0], sep, p[1])\n",
    "    \n",
    "    @_('ADVB complex_noun_phrase')\n",
    "    def adverbial_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.append(p[1])\n",
    "        return p[1]\n",
    "        \n",
    "    @_('ADJV_C noun_phrase')\n",
    "    def adjective_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[1]\n",
    "    \n",
    "    @_('determined_phrase opt_determination')\n",
    "    def noun_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        if p[1].text == '':\n",
    "            return p[0]\n",
    "        else:\n",
    "            return CombineMentions(p[0], ' ', p[1])\n",
    "\n",
    "    @_('undetermined_phrase')\n",
    "    def noun_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('SUBS_C determined_phrase')\n",
    "    def determined_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return CombineMentions(p[0], ' ', p[1])\n",
    "    \n",
    "    @_('determined_noun')\n",
    "    def determined_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('ART absolute_noun')\n",
    "    def determined_noun(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        m = CombineMentions(p[0], '', p[1])\n",
    "        # correct rpt for NP's starting with an article\n",
    "        m.rpt = p[1].rpt\n",
    "        return m \n",
    "    \n",
    "    @_('SUBS_A', 'NMPR')\n",
    "    def absolute_noun(self, p): \n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('PRS', 'NMPR', 'PRPS', 'PRDE')\n",
    "    def determined_noun(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "        \n",
    "    #for JHWH >DNJ, JHWH <LJWN etc.\n",
    "    @_('NMPR ADJV_A')\n",
    "    def determined_noun(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return CombineMentions(p[0], ' ', p[1])\n",
    "    \n",
    "    @_('SUBS_C undetermined_phrase')\n",
    "    def undetermined_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return CombineMentions(p[0], ' ', p[1])\n",
    "    \n",
    "    @_('undetermined_noun')\n",
    "    def undetermined_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "        \n",
    "    @_('SUBS_A', 'SUBS_P')\n",
    "    def undetermined_noun(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return p[0]\n",
    "    \n",
    "    @_('SUBS_A ADJV_A')\n",
    "    def undetermined_noun(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return CombineMentions(p[0], ' ', p[1])\n",
    "        \n",
    "    @_('PRIN')\n",
    "    def interrogative_phrase(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.discarded += 1\n",
    "        #print('discarding', p[0].text)\n",
    "        return Mention('', 0, 0)\n",
    "    \n",
    "    @_('')\n",
    "    def opt_determination(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        return Mention('', 0, 0)\n",
    "    \n",
    "    @_('ART PRDE', 'ART ADJV_A', 'ART SUBS_A')\n",
    "    def opt_determination(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        m = CombineMentions(p[0], '', p[1])\n",
    "        # correct rpt for NP's starting with an article\n",
    "        m.rpt = p[1].rpt\n",
    "        return m\n",
    "\n",
    "    @_('INTJ PRS', 'NEGA PRS', 'INRG PRS')\n",
    "    def subject_suffix(self, p):\n",
    "        self.statistics.rule_count[self.production.number] += 1\n",
    "        self.append(p[1])\n",
    "        return p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombineMentions(m1, sep, m2):\n",
    "    '''\n",
    "    Function is called in MyParser. \n",
    "    - Combines mentions that have been parsed from phrase atoms that contain complex\n",
    "    noun phrases or prepositional phrases. \n",
    "    - Assigns the person, gender and number of the first mention (`m1') to the whole mention construct. \n",
    "    - Assigns the issuffix boolean of m1 to the whole mention construct.\n",
    "    - Assigns the reconstructed phrase type (`rpt') of m1 to the whole mention construct.\n",
    "    - Assigns the function of m1 to the whole mention construct.\n",
    "    All assigned information is important for the coreference resolver. \n",
    "    '''\n",
    "    \n",
    "    node_tuple = (m1.node_tuple[0], m2.node_tuple[1])\n",
    "    text = m1.text + sep + m2.text\n",
    "    m = Mention(text, m1.start, m2.end, node_tuple)\n",
    "    m.person = m1.person\n",
    "    m.gender = m1.gender\n",
    "    m.number = m1.number \n",
    "    assert(not m1.issuffix)\n",
    "    m.issuffix = m1.issuffix\n",
    "    m.rpt = m1.rpt\n",
    "    assert(m1.function == m2.function)\n",
    "    m.function = m1.function\n",
    "    return m\n",
    "    \n",
    "def OpenErrorFile(clustername):\n",
    "    '''\n",
    "    Opens error analysis files in txt format in current folder\n",
    "    for the two phases of MiMi:\n",
    "    - Mention detection\n",
    "    - Coreference resolution\n",
    "    '''\n",
    "    \n",
    "    filename_mention_detection = f'mention_errors_{clustername}.out'\n",
    "    \n",
    "    md_errors = open(filename_mention_detection, 'w')\n",
    "    \n",
    "    return md_errors\n",
    "\n",
    "def OpenChapterFiles(filename):\n",
    "    '''\n",
    "    Opens for each chapter a text file with the Hebrew text \n",
    "    and a tsv file for Text-Fabric administration\n",
    "    in current folder. \n",
    "    '''\n",
    "    \n",
    "    filename_txt = f'{filename}.txt'\n",
    "    filename_tsv = f'{filename}.tsv'\n",
    "\n",
    "    txt_f = open(filename_txt, 'w')\n",
    "    tsv_f = open(filename_tsv, 'w')\n",
    "    \n",
    "    return txt_f, tsv_f\n",
    "\n",
    "def OpenAnn(filename):\n",
    "    '''\n",
    "    Opens an annotation file with extension `ann' for each chapter\n",
    "    in current folder. \n",
    "    '''\n",
    "    \n",
    "    filename_ann = f'{filename}.ann'\n",
    "    ann_f = open(filename_ann, 'w')\n",
    "    \n",
    "    return ann_f\n",
    "\n",
    "def make_index(offset, g_word):\n",
    "    '''\n",
    "    Produces the start and index of a transliterated Hebrew word. \n",
    "    Return an index tuple. \n",
    "    '''\n",
    "    \n",
    "    start = offset\n",
    "    end = offset + len(g_word) - 1\n",
    "    \n",
    "    return (start, end)\n",
    "\n",
    "def replace(g_prs):\n",
    "    '''\n",
    "    Strips the feature `g_prs' (inspect with: `F.g_prs.freqList()') of the pointed \n",
    "    representation of the pronominal suffix of a word in BHSA transliteration.\n",
    "    It returns the consonantal text of the suffix in string form. \n",
    "    Instead of '+@HEM', 'HM' is returned. \n",
    "    '''\n",
    "    \n",
    "    char_set = set('+:@.,;AEIOU')\n",
    "    if g_prs not in {'', '+'}:\n",
    "        new_prs = 'A' if g_prs == '+A' else ''.join(char for char in g_prs if char not in char_set)\n",
    "        return new_prs\n",
    "    \n",
    "def process_suffix(source_string, suffix, replace_what, replace_with):\n",
    "    '''\n",
    "    S.rpartition(sep) -> (head, sep, tail)\n",
    "\n",
    "    Search for the _sep (separator) in S (string), starting at the end of S, and return\n",
    "    the part before it, the separator itself, and the part after it.  If the\n",
    "    separator is not found, return two empty strings and S.\n",
    "    '''\n",
    "    if 'a' not in suffix:\n",
    "        if replace_what:\n",
    "            head, _sep, tail = source_string.rpartition(replace_what)\n",
    "        # non realised suffixes\n",
    "        else: \n",
    "            head = source_string\n",
    "            replace_with = '+'\n",
    "        return head + replace_with\n",
    "    else:\n",
    "        return source_string\n",
    "       \n",
    "def process_space(gcons_word, trailer, sep):\n",
    "    '''\n",
    "    Processes the space between transliterated Hebrew words for the text files. \n",
    "    If the feature trailer contains no space a hyphen is placed between the words. \n",
    "    In all other cases a space, specified in `sep' is generated.\n",
    "    '''\n",
    "    \n",
    "    if trailer == '':\n",
    "        return f'{gcons_word}-'\n",
    "    else:\n",
    "        return f'{gcons_word}{sep}'\n",
    "\n",
    "def emit_word(w, sep):\n",
    "    '''\n",
    "    Produces a transliterated Hebrew word (string), processed for spaces or trailers. \n",
    "    The suffix (g_prs) is processed by adding a '+' to make it more recognisable.\n",
    "    '''\n",
    "    \n",
    "    gcons_word = F.g_cons.v(w)\n",
    "    trailer = F.trailer.v(w)\n",
    "    g_prs = F.g_prs.v(w)\n",
    "    prs = F.prs.v(w)\n",
    "    new_prs = replace(g_prs)\n",
    "    g_word = process_suffix(gcons_word, prs, new_prs, f'+{new_prs}')\n",
    "    g_word = process_space(g_word, trailer, sep)\n",
    "    return g_word\n",
    "\n",
    "def WriteChapter(chapterNode, txt_f, tsv_f, filename):\n",
    "    '''\n",
    "    For each chapternode (integer), and thus for each Hebrew Bible chapter, two files are made:\n",
    "    1. A text file with the transliterated Hebrew text for import into brat. \n",
    "    The text files are made more readable by adding '-' between words that do not have a space\n",
    "    if there is a space it is retained. The suffix is given a '+'. \n",
    "    2. The tsv files contain for each word, separated by tabs: \n",
    "    start index, end index, word node, text of the word. \n",
    "    The `index_dict' is used for retrieving word indices to fill the Token object. \n",
    "    '''\n",
    "    \n",
    "    index_dict = {}\n",
    "    offset = 0\n",
    "    \n",
    "    def writeP(text, fh):\n",
    "        fh.write(text)\n",
    "        return len(text)\n",
    "    \n",
    "    offset += writeP(f'{filename}\\n', txt_f)\n",
    "\n",
    "    header = ['start_index', 'end_index', 'word_node', 'word']\n",
    "    tsv_f.write('{}\\n'.format('\\t'.join(header))) \n",
    "    \n",
    "    for vn in L.d(chapterNode, 'verse'):\n",
    "        \n",
    "        verse = T.sectionFromNode(vn)[2]\n",
    "        verse_words = L.d(vn, 'word')\n",
    "\n",
    "        # write transcription and .tsv\n",
    "        offset += writeP(f'{verse} ', txt_f)\n",
    "        \n",
    "        for i in range(len(verse_words)):\n",
    "            w = verse_words[i] \n",
    "            g_word = emit_word(w, ' ')\n",
    "            begin, end = make_index(offset, g_word)\n",
    "            index_dict[w] = (begin, end)\n",
    "            \n",
    "            tsv_f.write(f'{begin}\\t{end}\\t{w}\\t{g_word}\\n')\n",
    "            \n",
    "            offset += writeP(f'{g_word}', txt_f)\n",
    "    \n",
    "        offset += writeP('\\n', txt_f)\n",
    "\n",
    "    return index_dict\n",
    "\n",
    "def CloseChapterFiles(f1, f2): \n",
    "    '''\n",
    "    Close all txt and tsv file objects.\n",
    "    '''\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    \n",
    "def CloseFile(file):\n",
    "    '''\n",
    "    Close file object.\n",
    "    '''\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "def first_phrase_atom(pa):\n",
    "    '''\n",
    "    Check whether phrase atom is the first atom of the phrase.\n",
    "    '''\n",
    "    phrase = L.u(pa, 'phrase')[0]\n",
    "    return L.d(phrase, 'phrase_atom')[0] == pa\n",
    "    \n",
    "        \n",
    "def post_process(my_tokens):\n",
    "    '''\n",
    "    The function is called in `MakeTokens()'\n",
    "    The tokens that are made from a selection of phrase atoms as specified in constant\n",
    "    `TYP_CHOICE' are postprocessed in three cases:\n",
    "    1. complex prepositional phrases\n",
    "    2. substantives that are split from suffixes (`prs')\n",
    "    3. `EOA' (End Of Atom) tokens followed by a `CONJ' (conjunction) \n",
    "    '''\n",
    "    \n",
    "    for idx, item in enumerate(my_tokens):\n",
    "        \n",
    "        # Change token for complex prepositional phrases: CONJ PREP to CONJ_P PREP   \n",
    "        if item.type == 'PREP' and my_tokens[idx-1].type == 'CONJ':\n",
    "            my_tokens[idx-1].type = 'CONJ_P'\n",
    "        \n",
    "        # Make special token 'SUBS_P' for 'subs' split from prs \n",
    "        elif item.type == 'EOA' and my_tokens[idx-1].type == 'SUBS_C':\n",
    "             my_tokens[idx-1].type = 'SUBS_P'\n",
    "        \n",
    "        # if pattern is EOA CONJ delete CONJ\n",
    "        elif item.type == 'CONJ' and my_tokens[idx-1].type == 'EOA':\n",
    "             del my_tokens[idx]\n",
    "        \n",
    "    return my_tokens\n",
    "\n",
    "def test_eoa(my_tokens):\n",
    "    for idx, item in enumerate(my_tokens):\n",
    "        if item.type == 'EOA' and my_tokens[idx-1].type == 'EOA':\n",
    "            print('DEZE: ', my_tokens[idx-2].mention.text, my_tokens[idx-2].mention.node_tuple)\n",
    "        print(item.value.text, item.value.node_tuple)\n",
    "\n",
    "'''\n",
    "PTT is called in `MakeTokens()'\n",
    "phrase translation table. \n",
    "It translates phrase dependent part of speech \n",
    "to a corresponding phrase type. \n",
    "'''\n",
    "\n",
    "PTT = {'subs': 'NP',\n",
    "    'prep': 'PP',\n",
    "    'verb': 'VP',\n",
    "    'conj': 'CP',\n",
    "    'nmpr': 'PrNP',\n",
    "    'art': 'Art',\n",
    "    'nega': 'NegP',\n",
    "    'advb': 'AdvP',\n",
    "    'adjv': 'AdjP',\n",
    "    'prps': 'PPrP',\n",
    "    'prde': 'DPrP',\n",
    "    'intj': 'InjP',\n",
    "    'inrg': 'InrP',\n",
    "    'prin': 'InrP'\n",
    "}\n",
    "\n",
    "def split_prs(source_string, replace_what, replace_with):\n",
    "    '''\n",
    "    S.rpartition(sep) -> (head, sep, tail)\n",
    "    '''\n",
    "    \n",
    "    if replace_what:\n",
    "        head, _sep, tail = source_string.rpartition(replace_what)\n",
    "    else: \n",
    "        head = source_string\n",
    "        replace_with = '+'\n",
    "    return head, replace_with\n",
    "\n",
    "def print_tokens(token_list, path):\n",
    "    \n",
    "    f = open(path, 'a')\n",
    "    for t in token_list:\n",
    "        print(t.value.text, t.type, t.value.node_tuple, file=f)\n",
    "    f.close()\n",
    "\n",
    "def MakeTokens(chn, stats, index_dict):\n",
    "    '''\n",
    "    Retrieves all relevant phrase atom types, specified in `TYP_CHOICE' \n",
    "    that are needed for mention detection. The relevant information is stored \n",
    "    in `Mention' and added to `Token'. The tokens are appended to the list `MyTokens'. \n",
    "    \n",
    "    MakeTokens() first makes tokens of words that do not have a suffix. \n",
    "    If the pdp is not a substantive or adjective the pdp and status (pdp_status) is used for the token type. \n",
    "    \n",
    "    The words that have a suffix are split into a word and suffix part. The token for the word is pdp,\n",
    "    unless the word pdp is substantive or adjective, then the token is pdp_p. \n",
    "    Infinitives with predicate subject are treated as a finite verb, and are thus not split. \n",
    "    \n",
    "    After each phrase atom an EOA, End Of Atom, token, is made. \n",
    "    This is also the case for finite verbs and suffixes (not infc + PreS). \n",
    "    \n",
    "    `pa_count' counts the number of processed phrase atoms, this is important for keeping track of\n",
    "    how succesful the mention detection is. \n",
    "    '''\n",
    "    \n",
    "    MyTokens = []\n",
    "    pos = ''\n",
    "    prs = ''\n",
    "    gcons_word = ''\n",
    "    \n",
    "    for pa in L.d(chn, 'phrase_atom'):\n",
    "        langs = set(F.language.v(w) for w in L.d(pa, 'word'))\n",
    "        # check if language is Hebrew, and not Aramaic\n",
    "        if langs.issubset({'Hebrew'}):\n",
    "            pa_typ = F.typ.v(pa)\n",
    "            function = F.function.v(L.u(pa, 'phrase')[0])\n",
    "            # select preselected phrase atom types and phrase functions with subject suffix \n",
    "            if pa_typ in TYP_CHOICE or (function.endswith('S') and first_phrase_atom(pa)):\n",
    "                pa_words = L.d(pa, 'word')\n",
    "                pa_text = T.text(pa, fmt='text-trans-plain')\n",
    "                pa_word = tuple(word for word in pa_words)\n",
    "                for word in pa_word:\n",
    "                    pdp = F.pdp.v(word)\n",
    "                    status = F.st.v(word)\n",
    "                    #function = F.function.v(L.u(word, 'phrase')[0])\n",
    "                    vt = F.vt.v(word)\n",
    "                    gcons_word = F.g_cons.v(word)\n",
    "                    trailer = F.trailer.v(word)\n",
    "                    g_prs = F.g_prs.v(word)\n",
    "                    new_prs = replace(g_prs)\n",
    "                    ps = F.ps.v(word) # person\n",
    "                    gn = F.gn.v(word) # gender\n",
    "                    nu = F.nu.v(word) # number \n",
    "                    prs_ps = F.prs_ps.v(word) # person suffix\n",
    "                    prs_gn = F.prs_gn.v(word) # gender suffix \n",
    "                    prs_nu = F.prs_nu.v(word) # number suffix \n",
    "                    space_word = process_space(gcons_word, trailer, '')\n",
    "\n",
    "                    if 'a' in F.prs.v(word):\n",
    "                        if pdp not in {'subs', 'adjv'}:\n",
    "                            pos = f'{pdp}'.upper()\n",
    "                        else:\n",
    "                            pos = f'{pdp}_{status}'.upper()\n",
    "\n",
    "                        begin, end = index_dict[word]\n",
    "                        M = Mention(space_word, begin, end, (word, word), ps, gn, nu, False, PTT[pdp], function)\n",
    "                        MyTokens.append(Token(pos, M, word))\n",
    "\n",
    "                    else:\n",
    "                        prs = f'prs'.upper()\n",
    "                        g_word, plus_prs = split_prs(gcons_word, new_prs, f'+{new_prs}')\n",
    "\n",
    "                        # mention index administration for brat files based on input text files \n",
    "                        begin_word, end_prs = index_dict[word]\n",
    "                        begin_prs = end_word = begin_word + len(g_word)\n",
    "\n",
    "                        # every word with a status is tokenised seperately, except infc with PreS\n",
    "                        # only subs and adjv in construct when prs follows\n",
    "                        pos = f'{pdp}_p'.upper() if pdp in {'subs', 'adjv'} else f'{pdp}'.upper()\n",
    "                        M = Mention(g_word, begin_word, end_word, (word, word), ps, gn, nu, False, PTT[pdp], function)\n",
    "                        MyTokens.append(Token(pos, M, word))\n",
    "\n",
    "                        # filter out infinitives with predicate suffix\n",
    "                        if pdp in {'verb', 'subs'} and function != 'PreS':\n",
    "                            MyTokens.append(Token('EOA', Mention('*'), pa))\n",
    "                            stats.pa_count += 1\n",
    "                        M = Mention(plus_prs, begin_prs, end_prs, (word, word), prs_ps, prs_gn, prs_nu, True, 'PPrP', function)\n",
    "                        MyTokens.append(Token(prs, M, word))\n",
    "\n",
    "                        # add end of atom token after suffix if atom boundary is not reached\n",
    "                        if word != pa_word[-1]:\n",
    "                            MyTokens.append(Token('EOA', Mention('*'), pa))\n",
    "                            stats.pa_count += 1\n",
    "\n",
    "                MyTokens.append(Token('EOA', Mention('*'), pa))\n",
    "                stats.pa_count += 1\n",
    "    \n",
    "    # post processing \n",
    "    MyTokens = post_process(MyTokens)\n",
    "    #print_tokens(MyTokens, 'mytokensNew')\n",
    "    return MyTokens\n",
    "    \n",
    "def MentionParseStats(stats, my_book_name):\n",
    "    \n",
    "    # FIX: \n",
    "    # For Ezra, Isaiah and Psalms there is a difference between, (e.g. for the Psalms):\n",
    "    # - pa_success (18553) and stats.fc_parsed.count (18552) of -1\n",
    "    # - stats.error_count (46) and stats.fc_failed.count (47) of +1\n",
    "    # For all other books these counts are the same. \n",
    "    \n",
    "    mention_list = []\n",
    "    pa_success = stats.pa_count - stats.error_count #stats.fc_parsed.count\n",
    "    success_percent = round(stats.mention_success() *100, 1)\n",
    "    failure_percent = round(stats.mention_failure() *100, 1)\n",
    "    mention_corefs = stats.fc_parsed.total() + stats.fc_failed.total()\n",
    "    \n",
    "    print('\\n',\\\n",
    "        f'Mention Parse Statistics {my_book_name}: \\n',\\\n",
    "        f'{stats.pa_count} phrase atoms INPUT \\n', \\\n",
    "        f'{stats.fc_parsed.count} phrase atoms SUCCESFULLY parsed \\n',\\\n",
    "        f'+{stats.fc_parsed.bonus} extra mentions SUCCESFULLY parsed from phrase atoms \\n', \\\n",
    "        f'-{stats.fc_parsed.malus} phrase atoms without mentions \\n', \\\n",
    "        f'{stats.fc_failed.count} phrase atom parse ERRORS \\n',\\\n",
    "        f'+{stats.fc_failed.bonus} extra mentions SUCCESFULLY parsed from phrase atom errors \\n',\\\n",
    "        f'-{stats.fc_failed.malus} phrase atoms without mentions from phrase atom ERRORS \\n', \\\n",
    "        f'{mention_corefs} mention coreference input \\n', \\\n",
    "        f'{success_percent}% parsing succes \\n',\\\n",
    "        f'{failure_percent}% parsing error')\n",
    "    \n",
    "    mention_list.append({'book' : my_book_name,\n",
    "                                'phrase atoms' : stats.pa_count,\n",
    "                                'pa parsed' : stats.fc_parsed.count,\n",
    "                                '+m' : stats.fc_parsed.bonus, \n",
    "                                '-pa' : stats.fc_parsed.malus, \n",
    "                                'pa errors' : stats.fc_failed.count,\n",
    "                                '+m errors' : stats.fc_failed.bonus, \n",
    "                                '-pa errors' : stats.fc_failed.malus, \n",
    "                                'mentions': mention_corefs, \n",
    "                                '%parsed' : success_percent,\n",
    "                                '%error' : failure_percent   \n",
    "        })\n",
    "    \n",
    "    mention_stats_df = pd.DataFrame(mention_list)\n",
    "    mention_stats_df = mention_stats_df[['book', 'phrase atoms', 'pa parsed', '+m', '-pa', \n",
    "                                         'pa errors', '+m errors', '-pa errors', \n",
    "                                         'mentions', '%parsed', '%error']]\n",
    "    return mention_stats_df\n",
    "    \n",
    "def ProcessText(chn, filename): \n",
    "    '''\n",
    "    Write text and tsv files from chapter integer (nodes).\n",
    "    The chapter nodes are derived from my_book_name specified in CreateCoref()\n",
    "    '''\n",
    "    \n",
    "    txt_f, tsv_f = OpenChapterFiles(filename)\n",
    "    index_dict = WriteChapter(chn, txt_f, tsv_f, filename)\n",
    "    CloseChapterFiles(txt_f, tsv_f)\n",
    "    return index_dict\n",
    "\n",
    "def ParseMentions(chn, stats, index_dict):\n",
    "    '''\n",
    "    Executes MakeTokens(), MyLexer(), MyParser() \n",
    "    and returns a mentions list `Mentions' with mention objects.\n",
    "    '''\n",
    "    \n",
    "    Mentions = []\n",
    "    MyTokens = MakeTokens(chn, stats, index_dict)\n",
    "    \n",
    "    if len(MyTokens): \n",
    "        Lexer = MyLexer()\n",
    "        Parser = MyParser(stats)\n",
    "        Mentions = Parser.parse(Lexer.tokenize(MyTokens))\n",
    "    \n",
    "    return Mentions\n",
    "\n",
    "def PlaceMentions(mentions, ann_file):\n",
    "    '''\n",
    "    Relocates the mentions to the corresponding ann file. \n",
    "    '''\n",
    "    \n",
    "    i = 0\n",
    "    for m in mentions:\n",
    "        i += 1\n",
    "        m.name = f'T{str(i)}'\n",
    "        m.file = ann_file \n",
    "        ann_file.write(f'{m.name}{TAB}Mention {str(m.start)} {str(m.end)}{TAB}{m.text}{NL}')\n",
    "\n",
    "def EnrichMentions(mentions):\n",
    "    '''\n",
    "    Enrich mentions with: \n",
    "    - Text type: F.txt.v(clause)\n",
    "    - Paragraph number: F.pargr.v(clause_atom)\n",
    "    '''\n",
    "    \n",
    "    for m in mentions:\n",
    "        for clause in L.u(m.node_tuple[0], 'clause'):\n",
    "            m.txttype = F.txt.v(clause)\n",
    "            for clause_atom in L.d(clause, 'clause_atom'):\n",
    "                m.pargr = F.pargr.v(clause_atom)\n",
    "\n",
    "def MakeCorefSets(mentions):\n",
    "    '''\n",
    "    Initially MiMi processes a coreference list of singletons per text, \n",
    "    and merges them if they meet the criteria of the applied coreference sieves. \n",
    "    The coreference list of singleton sets is made here. \n",
    "    '''\n",
    "    \n",
    "    coref_list = []\n",
    "    for m in mentions:\n",
    "        m.corefclass = {m}\n",
    "        coref_list.append(m.corefclass)\n",
    "    return coref_list\n",
    "\n",
    "def CheckList(lst):\n",
    "    '''\n",
    "    Checks the integrity of the coreference list and prints it. The class feature\n",
    "    of the mentions in the sets should point to the right set.\n",
    "    '''\n",
    "    \n",
    "    for c in lst:\n",
    "        for m in c:\n",
    "            assert(m.corefclass == c)\n",
    "    pprint(lst)\n",
    "\n",
    "def Unite(lst, i1, i2):\n",
    "    '''\n",
    "    Unite(CorefList, i1, i2) is an operation on the CorefList. \n",
    "    The mentions from the class with index i2 are added to \n",
    "    those of i1. The class with index i2 is removed after \n",
    "    this operation. \n",
    "    '''\n",
    "    \n",
    "    if i1 != i2:\n",
    "        lst[i1] |= lst[i2]\n",
    "        for mention in lst[i2]:\n",
    "            mention.corefclass = lst[i1]\n",
    "        lst.pop(i2)\n",
    "        \n",
    "def find_clause(m):\n",
    "    '''\n",
    "    Iterate over clauses that contain the mention of choice:\n",
    "    e.g. mentions that have reconstructed phrase type 'Verbal Phrase'. \n",
    "    '''\n",
    "    return L.u(m.node_tuple[0], 'clause')[0]\n",
    "\n",
    "def mention_from_phrase(phrase, mentions):\n",
    "    '''\n",
    "    Return first mention word node(s) \n",
    "    contained within the phrase\n",
    "    '''\n",
    "    \n",
    "    words = L.d(phrase, 'word')\n",
    "    for m in mentions: \n",
    "        if m.node_tuple[0] in words:\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "def find_phrase_function(c, pf_set):\n",
    "    '''\n",
    "    Finds phrases with function from\n",
    "    phrase_function_set in clause c. \n",
    "    '''\n",
    "    \n",
    "    for p in L.d(c, 'phrase'):\n",
    "        if F.function.v(p) in pf_set:\n",
    "            return p\n",
    "    return None \n",
    "\n",
    "def suitable_mother(clause):\n",
    "    '''\n",
    "    Finds the mother clause of clauses that have a clause type\n",
    "    that ends with '0' within the same text domain (= feature F.txt.v()). \n",
    "    '''\n",
    "    \n",
    "    first_clause_atom = L.d(clause, 'clause_atom')[0]\n",
    "    mother_list = E.mother.f(first_clause_atom) or []\n",
    "    \n",
    "    clause_type = F.typ.v(clause)\n",
    "    if len(mother_list) and clause_type.endswith('0'): \n",
    "        mca = mother_list[0]\n",
    "        mother_clause = L.u(mca, 'clause')[0]\n",
    "        if F.txt.v(mother_clause) == F.txt.v(clause):\n",
    "            return mother_clause\n",
    "    return None\n",
    "\n",
    "def known(v):\n",
    "    '''\n",
    "    Returns values for person, gender, number that are known.\n",
    "    '''\n",
    "    return v != 'NA' and v != 'unknown'\n",
    "\n",
    "def disagreement(v1, v2):\n",
    "    '''\n",
    "    Checks for disagreement between returned values of known(). \n",
    "    '''\n",
    "    return known(v1) and known(v2) and (v1 != v2)\n",
    "    \n",
    "def do_agree(mother_pred_phrase, daughter_pred_phrase):\n",
    "    '''\n",
    "    Checks if the predicate phrase of the mother and daughter \n",
    "    agree. The words in the mother and daughter predicate with a specific \n",
    "    phrase dependent part of speech ('pdp_choice') need to agree. \n",
    "    '''\n",
    "    \n",
    "    pdp_choice = {'verb', 'subs', 'nmpr', 'prps', 'prde', 'adjv'}\n",
    "    mother_word = None \n",
    "    daughter_word = None \n",
    "    \n",
    "    for word in L.d(mother_pred_phrase, 'word'):\n",
    "        pdp = F.pdp.v(word)\n",
    "        if pdp in pdp_choice:\n",
    "            mother_word = word\n",
    "            break\n",
    "           \n",
    "    for word in L.d(daughter_pred_phrase, 'word'):\n",
    "        pdp = F.pdp.v(word)\n",
    "        if pdp in pdp_choice:\n",
    "            daughter_word = word\n",
    "            break\n",
    "    \n",
    "    # in a few books there is only a mother word \n",
    "    # and no daughter word (returns None), \n",
    "    # since the end of the hierarchy has been reached:\n",
    "    if mother_word and daughter_word:\n",
    "    \n",
    "        return not disagreement(F.ps.v(mother_word), F.ps.v(daughter_word)) and \\\n",
    "               not disagreement(F.nu.v(mother_word), F.nu.v(daughter_word)) and \\\n",
    "               not disagreement(F.gn.v(mother_word), F.gn.v(daughter_word))\n",
    "    \n",
    "def CheckCoref(corefs, statement):\n",
    "    n = 0\n",
    "    for c in corefs:\n",
    "        gn = ''\n",
    "        for m in c:\n",
    "            if m.gender in ['f', 'm']:\n",
    "                if gn == '':\n",
    "                    gn = m.gender\n",
    "                elif gn != m.gender:\n",
    "                    print(m.node_tuple, m.text, m.person, m.gender, m.number, sep='\\t')\n",
    "                    n+=1\n",
    "    if n > 0:\n",
    "        print(statement, n)\n",
    "                \n",
    "def ResolvePredicate(mentions, coref_list, stats, wv):\n",
    "    '''\n",
    "    MiMi's coreference resolution uses the predicate - subject relation as basis \n",
    "    for the rest of the sieves. \n",
    "    First mother and daughter clauses with an explicit subject phrase are retrieved. \n",
    "    If there is no subject phrase the function walks back to the mother clause,\n",
    "    checks if mother and daughter agree for person, number and gender, then unites them.\n",
    "    '''\n",
    "    \n",
    "    before = len(coref_list)\n",
    "    \n",
    "    for mpred in mentions:\n",
    "        if mpred.rpt == 'VP':\n",
    "            daughter_clause = find_clause(mpred)\n",
    "            subj_phrase = find_phrase_function(daughter_clause, {'Subj'})\n",
    "            if subj_phrase: \n",
    "                msubj = mention_from_phrase(subj_phrase, mentions)\n",
    "                if msubj:\n",
    "                    Unite(coref_list, coref_list.index(mpred.corefclass), \n",
    "                                 coref_list.index(msubj.corefclass))\n",
    "\n",
    "            else:\n",
    "                mother_clause = suitable_mother(daughter_clause)\n",
    "                daughter_pred_phrase = find_phrase_function(daughter_clause, \n",
    "                                                            {'Pred', 'PreC', 'PreO', 'PreS', 'PtcO'})\n",
    "                if mother_clause:\n",
    "                    mother_pred_phrase = find_phrase_function(mother_clause, {'Pred', 'PreO', 'PreS'})\n",
    "                    if mother_pred_phrase:\n",
    "                        mpredmother = mention_from_phrase(mother_pred_phrase, mentions)\n",
    "                        if mpredmother and do_agree(mother_pred_phrase, daughter_pred_phrase):\n",
    "                            Unite(coref_list, coref_list.index(mpred.corefclass), \n",
    "                                     coref_list.index(mpredmother.corefclass))\n",
    "    after = len(coref_list)\n",
    "    stats.resolve_predicate = before - after\n",
    "              \n",
    "        \n",
    "def mention_from_domain(mp, mentions):\n",
    "    '''\n",
    "    Checks if other mentions than mp (= mention person):\n",
    "    - are in the same paragraph as mp;\n",
    "    - has a higher node number than mp (meaning it appears later in the text);\n",
    "    - has the same grammatical person 1 or 2 as mp;\n",
    "    - if the reconstructed phrase type of mention is not a verbal phrase.\n",
    "    '''\n",
    "    \n",
    "    for m in mentions:\n",
    "        if \\\n",
    "        (m.pargr == mp.pargr) and \\\n",
    "        (m.node_tuple[0] > mp.node_tuple[0]) and \\\n",
    "        (m.person == mp.person) and \\\n",
    "        (m.rpt != 'VP'):\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "def ResolveFirstSecondPersonPronouns(mentions, coref_list, stats, wv):\n",
    "    '''\n",
    "    - Link 1P pronouns, `mp1', within same domain, except finite verbs\n",
    "    - Link 2P pronouns, `mp2', within same domain, except finite verbs\n",
    "    \n",
    "    To do: \n",
    "    - 3P: Walk back to first word group for which 3P, g, n are the same)\n",
    "    - Maybe: if mention in iteration has already been added to coref class:\n",
    "        - Unite 1P coref with predicate coref class\n",
    "    '''\n",
    "    \n",
    "    before = len(coref_list)\n",
    "    \n",
    "    mp1 = None\n",
    "    mp2 = None\n",
    "    \n",
    "    for m in mentions:\n",
    "        if m.person == 'p1' and m.rpt != 'VP':\n",
    "            mp1 = m\n",
    "            mp1_same_domain = mention_from_domain(mp1, mentions)\n",
    "            if mp1_same_domain:\n",
    "                \n",
    "                Unite(coref_list, coref_list.index(mp1.corefclass), \n",
    "                      coref_list.index(mp1_same_domain.corefclass))\n",
    "        \n",
    "        elif m.person == 'p2' and m.rpt != 'VP':\n",
    "            mp2 = m\n",
    "            \n",
    "            mp2_same_domain = mention_from_domain(mp2, mentions)\n",
    "            if mp2_same_domain:\n",
    "\n",
    "                Unite(coref_list, coref_list.index(mp2.corefclass), \n",
    "                      coref_list.index(mp2_same_domain.corefclass))\n",
    "                \n",
    "    after = len(coref_list)\n",
    "    stats.resolve_1p_2p_pronouns = before - after\n",
    "    \n",
    "def mention_from_vocative_domain(mvoct, mentions):\n",
    "    for m in mentions:\n",
    "        if \\\n",
    "        (m.pargr == mvoct.pargr) and \\\n",
    "        (m.txttype == mvoct.txttype) and \\\n",
    "        (m.node_tuple[0] > mvoct.node_tuple[0]) and \\\n",
    "        (m.person == 'p2'):\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "def ResolveVocative(mentions, coref_list, stats, wv):\n",
    "    ''' \n",
    "    If a mention has phrase function `Voct':\n",
    "        - Add all 2P mentions with same F.txt.v(clause) and F.pargr.v(clause_atom) \n",
    "        (= same domain) to vocative coreference class\n",
    "        - Unite vocative coref class with 2P predicate coref class\n",
    "    '''\n",
    "    before = len(coref_list)\n",
    "    \n",
    "    for m in mentions:\n",
    "        if m.function == 'Voct':\n",
    "            mvoct = m\n",
    "            mvoct_other = mention_from_vocative_domain(mvoct, mentions)\n",
    "            if mvoct_other and mvoct.corefclass != mvoct_other.corefclass:\n",
    "                \n",
    "                Unite(coref_list, coref_list.index(mvoct.corefclass), \n",
    "                      coref_list.index(mvoct_other.corefclass))\n",
    "                \n",
    "    after = len(coref_list)\n",
    "    stats.resolve_vocative = before - after \n",
    "    \n",
    "def find_phrase_atom(m):\n",
    "    return L.u(m.node_tuple[0], 'phrase_atom')[0]\n",
    "\n",
    "def find_phrase_atom_relation(phrase_atom, pa_rela):\n",
    "    rela = F.rela.v(phrase_atom)\n",
    "    if rela == pa_rela:\n",
    "        return phrase_atom\n",
    "    return None\n",
    "\n",
    "def mention_from_apposition(pa_appo, mentions):\n",
    "    '''\n",
    "    Return first mention word node(s) \n",
    "    contained within the phrase\n",
    "    '''\n",
    "    words = L.d(pa_appo, 'word')\n",
    "    for m in mentions: \n",
    "        if m.node_tuple[0] in words:\n",
    "            return m\n",
    "    return None\n",
    "    \n",
    "def mentions_from_preceding_phrase_atom(mappo, mentions):\n",
    "    '''\n",
    "    Finds the mention(s) that belongs to the \n",
    "    apposition relation in the mother phrase atom. \n",
    "    '''\n",
    "    \n",
    "    for m in mentions:\n",
    "        for pa in L.u(mappo.node_tuple[0], 'phrase_atom'):\n",
    "            mother_pa_list = E.mother.f(pa) or []\n",
    "            mpa = mother_pa_list[0]\n",
    "            pa_words = L.d(mpa, 'word')\n",
    "            if m.node_tuple[0] in pa_words:\n",
    "                return m\n",
    "    return None\n",
    "    \n",
    "def ResolveApposition(mentions, coref_list, stats, wv):\n",
    "    '''\n",
    "    Finds the mentions that have an apposition relation. \n",
    "    First find the mention with phrase_atom `rela' `appo', \n",
    "    then find the preceding mentions that belong to the apposition\n",
    "    through the mother phrase atom relation. \n",
    "    '''\n",
    "    \n",
    "    before = len(coref_list)\n",
    "    \n",
    "    for m in mentions:\n",
    "        phrase_atom = find_phrase_atom(m)\n",
    "        if phrase_atom:\n",
    "            pa_appo = find_phrase_atom_relation(phrase_atom, 'Appo')\n",
    "            if pa_appo:\n",
    "                mention_appo = mention_from_apposition(pa_appo, mentions)\n",
    "                mappo_mentions = mentions_from_preceding_phrase_atom(mention_appo, mentions)\n",
    "                if mappo_mentions and mention_appo.corefclass != mappo_mentions.corefclass:\n",
    "                    \n",
    "                    Unite(coref_list, coref_list.index(mention_appo.corefclass), \n",
    "                      coref_list.index(mappo_mentions.corefclass))\n",
    "                    \n",
    "    after = len(coref_list)\n",
    "    stats.resolve_apposition = before - after \n",
    "    \n",
    "def get_frnt_phrase(mfronted):\n",
    "    return L.u(mfronted.node_tuple[0], 'phrase')[0]\n",
    "\n",
    "def is_suffix_in(text_object):\n",
    "    for word in L.d(text_object, 'word'):\n",
    "        if F.prs.v(word) not in {'absent', 'n/a'}:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_resu_clause(clause_fronted):\n",
    "    \n",
    "    daughter_list = E.mother.t(clause_fronted) or []\n",
    "    if len(daughter_list):\n",
    "        dc = daughter_list[0]\n",
    "        clause_rela = F.rela.v(dc)\n",
    "        if clause_rela == 'Resu':\n",
    "            return dc\n",
    "    return None\n",
    "\n",
    "def find_resu_phrase(resu_clause):\n",
    "    for phrase in L.d(resu_clause, 'phrase'):\n",
    "        phrase_rela = F.rela.v(phrase)\n",
    "        if phrase_rela == 'Resu':\n",
    "            return phrase\n",
    "    return None\n",
    "\n",
    "def find_other_phrase(resu_clause, function, typ):\n",
    "    for phrase in L.d(resu_clause, 'phrase'):\n",
    "        phrase_function = F.function.v(phrase)\n",
    "        phrase_type = F.typ.v(phrase)\n",
    "        \n",
    "        if phrase_function == function and phrase_type == typ:\n",
    "            return phrase\n",
    "        elif phrase_function == function and typ == None:\n",
    "            return phrase\n",
    "    return None\n",
    "\n",
    "def find_suffix_in(text_object, mentions):\n",
    "    words = L.d(text_object, 'word')\n",
    "    for m in mentions:\n",
    "        if m.node_tuple[0] in words and m.issuffix:\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "def find_first_mention_in(frnt_phrase, mentions):\n",
    "    words = L.d(frnt_phrase, 'word')\n",
    "    for m in mentions:\n",
    "        if m.node_tuple[0] == words[0]:\n",
    "            return m\n",
    "    return None\n",
    "    \n",
    "def find_mention_in(phrase, mentions):\n",
    "    words = L.d(phrase, 'word')\n",
    "    for m in mentions:\n",
    "        if m.node_tuple[0] in words:\n",
    "            return m\n",
    "    return None\n",
    "        \n",
    "def ResolveFrontedElement(mentions, coref_list, stats, wv):\n",
    "    '''\n",
    "    * disclaimer: Resu has not been coded for large parts of the BHSA.\n",
    "    Heuristics were used. \n",
    "    \n",
    "    - Search for mentions that have phrase function fronted element `frnt'.\n",
    "    - Retrieve the clause that contains the fronted element. \n",
    "    - Retrieve the daughter of the fronted clause: resu clause\n",
    "    - Check in the resu clause for phrase relation `Resu' \n",
    "    - If not there, use heuristics. \n",
    "    '''\n",
    "    \n",
    "    before = len(coref_list)\n",
    "    \n",
    "    for m in mentions:\n",
    "        if m.function == 'Frnt': \n",
    "            frnt_phrase = get_frnt_phrase(m)\n",
    "            m = find_first_mention_in(frnt_phrase, mentions)\n",
    "            if m:\n",
    "                clause_fronted = find_clause(m)\n",
    "                resu_clause = find_resu_clause(clause_fronted)\n",
    "                if resu_clause:\n",
    "                    mfronted = m\n",
    "                    resu_phrase = find_resu_phrase(resu_clause)\n",
    "\n",
    "                    if resu_phrase:\n",
    "                        resu_mention = find_mention_in(resu_phrase, mentions)\n",
    "\n",
    "                        if resu_mention and not is_suffix_in(resu_phrase):\n",
    "                            Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                  coref_list.index(resu_mention.corefclass))\n",
    "                            \n",
    "                        elif resu_mention and is_suffix_in(resu_phrase):\n",
    "                            resu_suffix = find_suffix_in(resu_phrase, mentions)\n",
    "                            \n",
    "                            Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                  coref_list.index(resu_suffix.corefclass))\n",
    "\n",
    "                        elif resu_mention and resu_mention.issuffix:\n",
    "                            \n",
    "                            Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                  coref_list.index(resu_mention.corefclass))\n",
    "\n",
    "                    # heuristics for if there is no phrase relation `resu'\n",
    "                    else:\n",
    "                        #if (mfronted.rpt == 'PPrP' or mfronted.rpt == 'NP') and not mfronted.issuffix:\n",
    "                        prec_vp_phrase = find_other_phrase(resu_clause, 'PreC', 'VP')\n",
    "                        \n",
    "                        if mfronted.rpt == 'PPrP' and not mfronted.issuffix:\n",
    "                            mods_phrase = find_other_phrase(resu_clause, 'ModS', None)\n",
    "                            pres_vp_phrase = find_other_phrase(resu_clause, 'PreS', 'VP')\n",
    "                            preo_vp_phrase = find_other_phrase(resu_clause, 'PreO', 'VP')\n",
    "                            subj_phrase = find_other_phrase(resu_clause, 'Subj', None)\n",
    "                            cmpl_phrase = find_other_phrase(resu_clause, 'Cmpl', None)\n",
    "                            pred_vp_phrase = find_other_phrase(resu_clause, 'Pred', 'VP')\n",
    "                            \n",
    "                            # Subject suffix            \n",
    "                            # 5 cases Gen 44:14; 1Kgs 12:02, 20:40; Jer 33:01, 2Chr 34:03\n",
    "                            if mods_phrase: \n",
    "                                mods_mention = find_mention_in(mods_phrase, mentions)\n",
    "                                if mods_mention and mods_mention.issuffix:\n",
    "                                    \n",
    "                                    # check for agreement \n",
    "                                    if not disagreement(mfronted.person, mods_mention.person) and \\\n",
    "                                        not disagreement(mfronted.number, mods_mention.number) and \\\n",
    "                                        not disagreement(mfronted.gender, mods_mention.gender):\n",
    "                                        Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                              coref_list.index(mods_mention.corefclass))\n",
    "                            \n",
    "                            # Predicate object suffix \n",
    "                            # 3 cases EZE 30:18; HOS 08:06; ICHR09:22\n",
    "                            elif preo_vp_phrase:\n",
    "                                preo_vp_mention = find_mention_in(preo_vp_phrase, mentions)\n",
    "                                if preo_vp_mention and preo_vp_mention.issuffix:\n",
    "                                    \n",
    "                                    # check for agreement \n",
    "                                    if not disagreement(mfronted.person, preo_vp_mention.person) and \\\n",
    "                                        not disagreement(mfronted.number, preo_vp_mention.number) and \\\n",
    "                                        not disagreement(mfronted.gender, preo_vp_mention.gender):\n",
    "                                        Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                              coref_list.index(preo_vp_mention.corefclass))\n",
    "                            \n",
    "                            # Suffix on subject and complement\n",
    "                            # 3 cases EZE 33:17; QOH 07:26; THR 01:04\n",
    "                            elif subj_phrase:\n",
    "                                subj_mention = find_mention_in(subj_phrase, mentions)\n",
    "                                if subj_mention and subj_mention.issuffix:\n",
    "                                    \n",
    "                                    # check for agreement \n",
    "                                    if not disagreement(mfronted.person, subj_mention.person) and \\\n",
    "                                        not disagreement(mfronted.number, subj_mention.number) and \\\n",
    "                                        not disagreement(mfronted.gender, subj_mention.gender):\n",
    "                                        Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                              coref_list.index(subj_mention.corefclass))                   \n",
    "                            elif cmpl_phrase:\n",
    "                                cmpl_mention = find_mention_in(cmpl_phrase, mentions)\n",
    "                                if cmpl_mention and cmpl_mention.issuffix:\n",
    "                                    \n",
    "                                    # check for agreement \n",
    "                                    if not disagreement(mfronted.person, cmpl_mention.person) and \\\n",
    "                                        not disagreement(mfronted.number, cmpl_mention.number) and \\\n",
    "                                        not disagreement(mfronted.gender, cmpl_mention.gender):\n",
    "                                        Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                              coref_list.index(cmpl_mention.corefclass))\n",
    "                            \n",
    "                            # Predicate and subject \n",
    "                            # 3 cases GEN 40,10; ISAM 20:29; ISA 57:06        \n",
    "                            elif pred_vp_phrase and subj_phrase:\n",
    "                                pred_vp_mention = find_mention_in(pred_vp_phrase, mentions)\n",
    "                                subj_mention = find_mention_in(subj_phrase, mentions)\n",
    "                                if pred_vp_mention:\n",
    "                                    \n",
    "                                    Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                      coref_list.index(pred_vp_mention.corefclass))\n",
    "                                if subj_mention.issuffix:\n",
    "                                    \n",
    "                                    Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                      coref_list.index(subj_mention.corefclass))\n",
    "                            \n",
    "                            # Only predicate\n",
    "                            # 4 cases Eze 02:05, 04:12; HAB 01:10 2x       \n",
    "                            elif pred_vp_phrase:\n",
    "                                pred_vp_mention = find_mention_in(pred_vp_phrase, mentions)\n",
    "                                if pred_vp_mention:\n",
    "                                    \n",
    "                                    Unite(coref_list, coref_list.index(mfronted.corefclass), \n",
    "                                      coref_list.index(pred_vp_mention.corefclass))\n",
    "\n",
    "    after = len(coref_list)\n",
    "    stats.resolve_fronted = before - after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronoun(w):\n",
    "    '''\n",
    "    Boolean function that checks for the existence of a pronoun.\n",
    "    '''\n",
    "    return F.pdp.v(w) in {'prps', 'prde', 'prin'}\n",
    "        \n",
    "def is_kol(w):\n",
    "    '''\n",
    "    Boolean function that chscks if 'KL/' is head of a phrase.\n",
    "    '''\n",
    "    return F.lex.v(w) == 'KL/' and F.st.v(w) == 'c'\n",
    "    \n",
    "def phrase_head(m):\n",
    "    '''\n",
    "    Finds the head of the phrase in which the mention (`m`)\n",
    "    is contained. The head is found by searching \n",
    "    for the first word in the phrase \n",
    "    with a nominal ending (`nme`). Of that word \n",
    "    the lexeme is returned. \n",
    "    '''\n",
    "    \n",
    "    phrase = L.u(m.node_tuple[0], 'phrase')[0]\n",
    "    for word in L.d(phrase, 'word'):\n",
    "        if m.issuffix:\n",
    "            return F.prs.v(word)\n",
    "        elif F.nme.v(word) not in {'n/a', 'absent'} and not is_kol(word) or pronoun(word):\n",
    "            return F.lex.v(word)\n",
    "    return None\n",
    "\n",
    "def match_head(m1, m2):\n",
    "    '''\n",
    "    The function phrase_head returns a lexeme or None. \n",
    "    If `lex1` (lexeme of `m1`) and `lex2` (lexeme of `m2`) \n",
    "    have a value a boolean is returned if \n",
    "    lex1 and lex2 match on lexeme. \n",
    "    The lexemes cannot be contained in the same phrase. \n",
    "    This is checked with `phr1` and `phr1`. \n",
    "    '''\n",
    "\n",
    "    lex1 = phrase_head(m1)\n",
    "    lex2 = phrase_head(m2)\n",
    "    phr1 = L.u(m1.node_tuple[0], 'phrase')[0]\n",
    "    phr2 = L.u(m2.node_tuple[0], 'phrase')[0]\n",
    "    \n",
    "    # filter out mentions that are contained within the same phrase\n",
    "    return phr1 != phr2 and lex1 and lex2 and lex1 == lex2\n",
    "    \n",
    "def distance_component(d):\n",
    "    '''\n",
    "    Calculates the distance component of words in a text\n",
    "    of the feature vector match_vector with log. \n",
    "    `d`,integer, is the distance in word nodes between words. \n",
    "    The 0.4 is a scaling factor has been radomnly chosen, \n",
    "    it decreases the log(d) value for readability. \n",
    "    '''\n",
    "    v = 0.4 * math.log(d)\n",
    "    return v\n",
    "    \n",
    "def vector_length(match_vector):\n",
    "    '''\n",
    "    Calculates and returns the euclidic length of the list `match_vector`.\n",
    "    `match_vector` contains a number of components. \n",
    "    Each component is a numeric value that represents \n",
    "    the amount of agreement between two mentions for a specific feature. \n",
    "    `vector_length` is called in `score_match`. \n",
    "    '''\n",
    "    i_sum = 0\n",
    "    euclidic = 0\n",
    "    for i in match_vector:\n",
    "        i_sqrt = i**2\n",
    "        i_sum += i_sqrt\n",
    "        euclidic = math.sqrt(i_sum)  \n",
    "    return euclidic\n",
    "\n",
    "def score_match(m1, m2, w):\n",
    "    '''\n",
    "    Function scores the match between two mentions \n",
    "    `m1` and `m2` that are compared for agreement. \n",
    "    `w` is a (list) vector with weights\n",
    "    per feature that is compared. `w` can be adjusted in\n",
    "    the sieves MatchString(), ResolveEntity(), \n",
    "    ResolveThirdPersonPronouns().\n",
    "    Funtions returns a score which is the result\n",
    "    of the product of each element in match_vector and `w`. \n",
    "    '''\n",
    "    match_vector = []\n",
    "    txt_distance = m1.node_tuple[0] - m2.node_tuple[0]\n",
    "    # distance in words with math.log(d)\n",
    "    d = distance_component(txt_distance)\n",
    "    match_vector.append(d)\n",
    "    match_vector.append(disagreement(m1.gender, m2.gender))\n",
    "    match_vector.append(disagreement(m1.number, m2.number))\n",
    "    w = list(w)\n",
    "    w.insert(0, 1.0) # insert value 1.0 to weigh the distance = [1] + w\n",
    "    w = tuple(w)\n",
    "    \n",
    "    return vector_length(np.multiply(w, match_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchString(mentions, coref_list, stats, wv):\n",
    "    '''\n",
    "    Matches lexeme strings (not suffixes) exactly by finding for each \n",
    "    singleton mention the best possible candidate \n",
    "    mention in a class. The best candidate is found by:\n",
    "    - walking backwards in the text to the beginning;\n",
    "    - assigning a match score between the two compared mentions (m1, m2);\n",
    "    - the score is calculated with a feature weights vector and the length\n",
    "    of the match vector;\n",
    "    - all canditate mentions for m1 are stored in a heap;\n",
    "    - the best mention m2 with the best score is popped from the heap\n",
    "    and united with m1;\n",
    "    - after the push, a new heap is made for a new mention singleton.\n",
    "    '''\n",
    "    before = len(coref_list) \n",
    "    feature_weights = [1.0, 1.0]\n",
    "    unite = []\n",
    "    for m in mentions:\n",
    "        if len(m.corefclass) == 1 and m.rpt != 'PrNP' and not m.issuffix:\n",
    "            heap = []\n",
    "            for clss in coref_list:\n",
    "                sorted_clss = sorted(clss, key=attrgetter('node_tuple'))\n",
    "                i = 0\n",
    "                while i < len(sorted_clss) and m > sorted_clss[i]:\n",
    "                    m2 = sorted_clss[i]\n",
    "                    if not m2.issuffix and match_head(m, m2):\n",
    "                        score = score_match(m, m2, feature_weights)\n",
    "                        heappush(heap, (score, m2)) #list of tuples\n",
    "                    i += 1\n",
    "            \n",
    "            if len(heap) > 0:\n",
    "                best_match = heappop(heap)\n",
    "                Unite(coref_list, coref_list.index(m.corefclass), \n",
    "                      coref_list.index(best_match[1].corefclass))\n",
    "                #print('united strings', (m.text, m.node_tuple), (best_match[1].text, best_match[1].node_tuple), '\\n')\n",
    "                unite.append((m.text, best_match[1].text))\n",
    "    #print('unite_string_list', len(unite), unite, '\\n')                   \n",
    "    \n",
    "    after = len(coref_list)\n",
    "    stats.match_string = before - after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResolveEntity(mentions, coref_list, stats, wv):\n",
    "    '''\n",
    "    Resolves named entities as coded in the BSHA data by finding for each \n",
    "    singleton mention the best possible candidate \n",
    "    mention in a class. The best candidate is found by:\n",
    "    - walking backwards in the text to the beginning;\n",
    "    - assigning a match score between the two compared mentions (m1, m2);\n",
    "    - the score is calculated with a feature weights vector and the length\n",
    "    of the match vector;\n",
    "    - all canditate mentions for m1 are stored in a heap;\n",
    "    - the best mention m2 with the best score is pushed from the heap;\n",
    "    and united with m1;\n",
    "    - after the push, a new heap is made for a new mention singleton.\n",
    "    '''\n",
    "    before = len(coref_list)\n",
    "    \n",
    "    feature_weights = [0.0, 0.0]\n",
    "    unite = []\n",
    "    for m in mentions:\n",
    "        if len(m.corefclass) == 1 and m.rpt == 'PrNP':\n",
    "            heap = []\n",
    "            for clss in coref_list:\n",
    "                sorted_clss = sorted(clss, key=attrgetter('node_tuple'))\n",
    "                i = 0\n",
    "                while i < len(sorted_clss) and m > sorted_clss[i]:\n",
    "                    m2 = sorted_clss[i]\n",
    "                    if m2.rpt == 'PrNP' and match_head(m, m2):\n",
    "                        score = score_match(m, m2, feature_weights)\n",
    "                        heappush(heap, (score, m2)) #list of tuples\n",
    "                    i += 1\n",
    "                    \n",
    "            if len(heap) > 0:\n",
    "                best_match = heappop(heap)\n",
    "                Unite(coref_list, coref_list.index(m.corefclass), \n",
    "                      coref_list.index(best_match[1].corefclass))\n",
    "                #print('united entities', (m.text, m.node_tuple), (best_match[1].text, best_match[1].node_tuple), '\\n')\n",
    "                unite.append((m.text, best_match[1].text))\n",
    "    #print('unite_entity_list', len(unite), unite, '\\n')    \n",
    "          \n",
    "    after = len(coref_list)\n",
    "    stats.resolve_entity = before - after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_match_3p(m1, m2, w):\n",
    "    '''\n",
    "    Function scores the match between two mentions \n",
    "    `m1` and `m2` that are compared for agreement. \n",
    "    `w` is a (list) vector with weights\n",
    "    per feature that is compared. `w` can be adjusted in\n",
    "    the sieve ResolveThirdPersonPronouns. \n",
    "    Funtion returns a score which is the result\n",
    "    of the product of each element in match_vector and `w`. \n",
    "    '''\n",
    "    match_vector = []\n",
    "    txt_distance = m1.node_tuple[0] - m2.node_tuple[0]\n",
    "    # distance in words with math.log(d)\n",
    "    d = distance_component(txt_distance)\n",
    "    match_vector.append(d)\n",
    "    \n",
    "    match_vector.append(disagreement(m1.gender, m2.gender))\n",
    "    match_vector.append(disagreement(m1.number, m2.number))\n",
    "    \n",
    "    w = list(w)\n",
    "    w.insert(0, 1.0) # insert value 1.0 to weigh the distance\n",
    "    w = tuple(w)\n",
    "    \n",
    "    return vector_length(np.multiply(w, match_vector))\n",
    "\n",
    "def ResolveThirdPersonPronouns(mentions, coref_list, stats, wv):\n",
    "    '''\n",
    "    Resolves third person pronouns by finding for each \n",
    "    singleton 3p pronoun mention the best possible candidate \n",
    "    mention in a class. The best candidate is found by:\n",
    "    - walking backwards in the text to the beginning;\n",
    "    - assigning a match score between the two compared mentions (m1, m2);\n",
    "    - the score is calculated with a feature weights vector and the length\n",
    "    of the match vector;\n",
    "    - all canditate mentions for m1 are stored in a heap;\n",
    "    - the best mention m2 with the best score is pushed from the heap;\n",
    "    and united with m1;\n",
    "    - after the push, a new heap is made for a new mention singleton.\n",
    "    '''\n",
    "    before = len(coref_list)\n",
    "    feature_weights = [2.0, 3.6] #[0.002403569522293084, 0.5033652397875054]\n",
    "    unite = []\n",
    "    for m in mentions:\n",
    "        if len(m.corefclass) == 1 and m.person == 'p3': #issuffix\n",
    "            heap = []\n",
    "            for clss in coref_list:\n",
    "                sorted_clss = sorted(clss, key=attrgetter('node_tuple'))\n",
    "                i = 0\n",
    "                while i < len(sorted_clss) and m > sorted_clss[i]:\n",
    "                    m2 = sorted_clss[i]\n",
    "                    if m2.person == 'p3':\n",
    "                        score = score_match(m, m2, feature_weights)\n",
    "                        heappush(heap, (score, m2)) #list of tuples\n",
    "                    i += 1\n",
    "            \n",
    "            if len(heap) > 0:\n",
    "                best_match = heappop(heap)\n",
    "                Unite(coref_list, coref_list.index(m.corefclass), \n",
    "                      coref_list.index(best_match[1].corefclass))\n",
    "                #print('united 3p pronouns', (m.text, m.node_tuple), (best_match[1].text, best_match[1].node_tuple), '\\n')\n",
    "                unite.append((m.text, best_match[1].text))\n",
    "    #print('unite_3p_list', len(unite), unite, '\\n')\n",
    "    \n",
    "    after = len(coref_list)\n",
    "    stats.resolve_3p_pronouns = before - after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeSieveList():\n",
    "    \n",
    "    '''\n",
    "    Appends sieve functions to sieve_list.\n",
    "    The function ExecuteSieves(sieve_list, mentions, corefs) \n",
    "    executes the sieves. \n",
    "    '''\n",
    "    \n",
    "    sieve_list = []\n",
    "    sieve_list.append(ResolveFirstSecondPersonPronouns)\n",
    "    sieve_list.append(ResolveVocative)\n",
    "    sieve_list.append(MatchString)\n",
    "    sieve_list.append(ResolveEntity)\n",
    "    sieve_list.append(ResolveApposition)\n",
    "    sieve_list.append(ResolveFrontedElement)\n",
    "    sieve_list.append(ResolvePredicate)\n",
    "    sieve_list.append(ResolveThirdPersonPronouns)\n",
    "    \n",
    "    return sieve_list\n",
    "\n",
    "def ExecuteSieves(sieve_list, mentions, corefs, stats, wv):\n",
    "    '''\n",
    "    Executes sieves in sieve_list:\n",
    "    ResolvePredicate(mentions, corefs)\n",
    "    ResolveFirstSecondPersonPronouns(mentions, corefs)\n",
    "    ResolveVocative(mentions, corefs)\n",
    "    ResolveApposition(mentions, corefs)\n",
    "    ResolveFrontedElement(mentions, corefs)\n",
    "    ResolveEntity(mentions, corefs)\n",
    "    MatchString(mentions, corefs)\n",
    "    ResolveThirdPersonPronouns(mentions, corefs)\n",
    "    '''\n",
    "    \n",
    "    for sieve in sieve_list:\n",
    "        sieve(mentions, corefs, stats, wv)\n",
    "\n",
    "def PlaceCoref(mentions, corefs, ann_file):\n",
    "    '''\n",
    "    Writes a coref class with mentions to .ann file object.\n",
    "    sorted_coref_lists contains the mention identifiers (e.g. T50)\n",
    "    sorted on TF node. \n",
    "    '''\n",
    "    \n",
    "    for s in corefs:\n",
    "        if len(s) > 1:\n",
    "            ann_file.write(f'*{TAB}Coreference')\n",
    "            sorted_coref_lists = sorted(s, key=attrgetter('node_tuple'))\n",
    "            for m in sorted_coref_lists:\n",
    "                ann_file.write(f' {m}')\n",
    "            ann_file.write('\\n')\n",
    "\n",
    "def CountCorefClasses(stats, corefs):\n",
    "    i = 0\n",
    "    for s in corefs:\n",
    "        if len(s) > 1:\n",
    "            i+=1\n",
    "    stats.coref_classes = i\n",
    "    \n",
    "def FindMentionByRPT(c, typ):\n",
    "    for m in c:\n",
    "        if m.rpt == typ:\n",
    "            return m\n",
    "    \n",
    "def Identify(c):\n",
    "    rpt_order = ['PrNP', 'NP', 'PtcP', 'VP', 'PPrP', 'DPrP']\n",
    "    for typ in rpt_order:\n",
    "        m = FindMentionByRPT(c, typ)\n",
    "        if m:\n",
    "            m.who = m.text\n",
    "            return\n",
    "    #c.who = c.first()\n",
    "            \n",
    "def AssignWho(corefs):\n",
    "    for c in corefs:\n",
    "        if len(c) > 1:\n",
    "             m = Identify(c)\n",
    "            \n",
    "def CorefResolutionStats(stats, filename, coreference_list):\n",
    "\n",
    "    resolved_corefs = stats.input_corefs - stats.output_corefs\n",
    "    coref_success_percent = round(stats.coref_success() * 100, 1)\n",
    "    coref_unresolved_percent = round(stats.coref_unresolved() * 100, 1)\n",
    "    \n",
    "    coreference_list.append({'chapter' : filename,\n",
    "                                 'input corefs' : stats.input_corefs,\n",
    "                                 'resolved' : resolved_corefs,\n",
    "                                 'unresolved' : stats.output_corefs,\n",
    "                                 '%resolved' : coref_success_percent,\n",
    "                                 '%unresolved' : coref_unresolved_percent,\n",
    "                                 'classes' : stats.coref_classes\n",
    "        })\n",
    "    \n",
    "    coref_stats_df = pd.DataFrame(coreference_list)\n",
    "    coref_stats_df = coref_stats_df[['chapter', 'input corefs', 'resolved', 'unresolved', \n",
    "                                     '%resolved', '%unresolved', 'classes']]\n",
    "    \n",
    "    return coref_stats_df\n",
    "    \n",
    "def SumCoResStats(total_stats, my_book_name):\n",
    "    \n",
    "    coreference_list = []\n",
    "    resolved_corefs = total_stats.input_corefs - total_stats.output_corefs\n",
    "    coref_success_percent = round(total_stats.coref_success() * 100, 1)\n",
    "    coref_unresolved_percent = round(total_stats.coref_unresolved() * 100, 1)\n",
    "    \n",
    "    print('\\n',\\\n",
    "        f'Coreference Resolution Statistics {my_book_name}: \\n',\\\n",
    "        f'{total_stats.input_corefs} total input corefs \\n', \\\n",
    "        f'{resolved_corefs} corefs RESOLVED \\n',\\\n",
    "        f'{total_stats.output_corefs} corefs UNRESOLVED \\n',\\\n",
    "        f'{coref_success_percent}% corefs RESOLVED \\n',\\\n",
    "        f'{coref_unresolved_percent}% corefs UNRESOLVED \\n',\\\n",
    "        f'{total_stats.coref_classes} classes')\n",
    "    \n",
    "    coreference_list.append({'book' : my_book_name,\n",
    "                             'input corefs' : total_stats.input_corefs,\n",
    "                             'resolved' : resolved_corefs,\n",
    "                             'unresolved' : total_stats.output_corefs,\n",
    "                             '%resolved' : coref_success_percent,\n",
    "                             '%unresolved' : coref_unresolved_percent,\n",
    "                             'classes' : total_stats.coref_classes\n",
    "        })\n",
    "    \n",
    "    coref_total_stats_df = pd.DataFrame(coreference_list)\n",
    "    coref_total_stats_df = coref_total_stats_df[['book', 'input corefs', 'resolved', 'unresolved', \n",
    "                                     '%resolved', '%unresolved', 'classes']]\n",
    "    \n",
    "    return coref_total_stats_df\n",
    "\n",
    "def SieveStats(stats, filename, sieves_list):\n",
    "    \n",
    "    resolve_total = stats.resolve_predicate + stats.resolve_1p_2p_pronouns + \\\n",
    "    stats.resolve_vocative + stats.resolve_apposition + stats.resolve_fronted + \\\n",
    "    stats.resolve_entity + stats.match_string + stats.resolve_3p_pronouns\n",
    "    \n",
    "    sieves_list.append({'chapter' : filename,\n",
    "                        '1p 2p pronoun' : stats.resolve_1p_2p_pronouns,\n",
    "                        'vocative' : stats.resolve_vocative,\n",
    "                        'string' : stats.match_string,\n",
    "                        'entity' : stats.resolve_entity,\n",
    "                        'apposition' : stats.resolve_apposition,\n",
    "                        'fronted element' : stats.resolve_fronted,\n",
    "                        'predicate' : stats.resolve_predicate,\n",
    "                        '3p pronoun' : stats.resolve_3p_pronouns,\n",
    "                        'total sieves' : resolve_total,\n",
    "                        'classes' : stats.coref_classes \n",
    "        })\n",
    "        \n",
    "    sieve_stats_df = pd.DataFrame(sieves_list)\n",
    "    sieve_stats_df = sieve_stats_df[['chapter', '1p 2p pronoun',\n",
    "                                     'vocative', 'string', 'entity',\n",
    "                                     'apposition', 'fronted element',\n",
    "                                     'predicate', '3p pronoun',\n",
    "                                     'total sieves', 'classes']]\n",
    "    return sieve_stats_df\n",
    "\n",
    "def SumSieveStats(total_stats, book_name):\n",
    "    '''\n",
    "    Sum of unite operations in sieves. \n",
    "    '''\n",
    "    sieves_list = []\n",
    "    \n",
    "    resolve_total = total_stats.resolve_predicate + total_stats.resolve_1p_2p_pronouns + \\\n",
    "    total_stats.resolve_vocative + total_stats.resolve_apposition + \\\n",
    "    total_stats.resolve_fronted + total_stats.resolve_entity + \\\n",
    "    total_stats.match_string + total_stats.resolve_3p_pronouns\n",
    "    \n",
    "    sieves_list.append({'book' : book_name,\n",
    "                        '1p 2p pronoun' : total_stats.resolve_1p_2p_pronouns,\n",
    "                        'vocative' : total_stats.resolve_vocative,\n",
    "                        'string' : total_stats.match_string,\n",
    "                        'entity' : total_stats.resolve_entity,\n",
    "                        'apposition' : total_stats.resolve_apposition,\n",
    "                        'fronted element' : total_stats.resolve_fronted,\n",
    "                        'predicate' : total_stats.resolve_predicate,\n",
    "                        '3p pronoun' : total_stats.resolve_3p_pronouns,\n",
    "                        'total sieves' : resolve_total,\n",
    "                        'classes' : total_stats.coref_classes \n",
    "       })\n",
    "     \n",
    "    sieve_total_stats_df = pd.DataFrame(sieves_list)\n",
    "    sieve_total_stats_df = sieve_total_stats_df[['book', '1p 2p pronoun',\n",
    "                                                 'vocative', 'string', 'entity',\n",
    "                                                 'apposition', 'fronted element',\n",
    "                                                 'predicate', '3p pronoun',\n",
    "                                                 'total sieves', 'classes']]                  \n",
    "                       \n",
    "    print('\\n',\\\n",
    "          f'Sieve Statistics {book_name}: \\n',\\\n",
    "          f'1p 2p Pronoun Sieve: {total_stats.resolve_1p_2p_pronouns} \\n',\\\n",
    "          f'Vocative Sieve: {total_stats.resolve_vocative} \\n',\\\n",
    "          f'String Sieve: {total_stats.match_string} \\n',\\\n",
    "          f'Entity Sieve: {total_stats.resolve_entity} \\n',\\\n",
    "          f'Apposition Sieve: {total_stats.resolve_apposition} \\n',\\\n",
    "          f'Fronted Element Sieve: {total_stats.resolve_fronted} \\n',\\\n",
    "          f'Predicate Sieve: {total_stats.resolve_predicate} \\n',\\\n",
    "          f'3p Pronoun Sieve: {total_stats.resolve_3p_pronouns} \\n',\\\n",
    "          f'Total Sieves: {resolve_total} \\n',\\\n",
    "          f'Total Classes: {total_stats.coref_classes}'\n",
    "         )\n",
    "    \n",
    "    return sieve_total_stats_df\n",
    "\n",
    "def PrintMentions(Mentions):\n",
    "    for m in Mentions:\n",
    "        print(m.name, m.txttype, m.pargr, m.text, m.person, m.gender, \n",
    "              m.number, m.issuffix, m.function, m.rpt, m.node_tuple[0],\n",
    "              sep='\\t'\n",
    "             )\n",
    "\n",
    "def PrintCorefClasses(Corefs):\n",
    "    i = 0 \n",
    "    for s in Corefs:\n",
    "        if len(s) > 1:\n",
    "            i+=1\n",
    "            print(f'C{i}', sorted(s, key=attrgetter('node_tuple')))\n",
    "        if len(s) == 1:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimi(my_book_name, first_chapter, last_chapter, wv):\n",
    "    print('Start.')\n",
    "    clustername = f'{my_book_name}_{first_chapter:>03}_{last_chapter:>03}'\n",
    "    global mention_errors \n",
    "    mention_errors = OpenErrorFile(clustername)\n",
    "    total_stats = Statistics()\n",
    "    coreference_list = []\n",
    "    sieves_list = []\n",
    "    i = 0\n",
    "    coref_lst = []\n",
    "    for chapter_number in range(first_chapter, last_chapter+1):\n",
    "        stats = Statistics()\n",
    "        chn = T.nodeFromSection((my_book_name, chapter_number))\n",
    "        if chn == None:\n",
    "            error(f'Chapter not found')\n",
    "            break\n",
    "        filename = f'{my_book_name}_{chapter_number:>03}'\n",
    "        index_dict = ProcessText(chn, filename)\n",
    "        Mentions = ParseMentions(chn, stats, index_dict)\n",
    "        ann_file = OpenAnn(filename)\n",
    "        PlaceMentions(Mentions, ann_file)\n",
    "        EnrichMentions(Mentions)\n",
    "        Corefs = MakeCorefSets(Mentions)\n",
    "        stats.input_corefs = len(Corefs)\n",
    "        sieve_list = MakeSieveList()\n",
    "        ExecuteSieves(sieve_list, Mentions, Corefs, stats, wv)\n",
    "        stats.output_corefs = len(Corefs)\n",
    "        PlaceCoref(Mentions, Corefs, ann_file)\n",
    "        CloseFile(ann_file)\n",
    "        AssignWho(Corefs)\n",
    "        \n",
    "        # statistics\n",
    "        CountCorefClasses(stats, Corefs)\n",
    "        sieve_stats_df = SieveStats(stats, filename, sieves_list)\n",
    "        coref_stats_df = CorefResolutionStats(stats, filename, coreference_list)\n",
    "        total_stats += stats\n",
    "        coref_lst.append(Corefs)\n",
    "        #CheckList(Corefs)\n",
    "    mention_stats_df = MentionParseStats(total_stats, my_book_name)\n",
    "    #print('Parsing and resolution: done.')\n",
    "    CloseFile(mention_errors)\n",
    "    \n",
    "    ##PrintMentions(Mentions) #For GoMiMi()\n",
    "    ##CheckList(Corefs) #For GoMiMi()\n",
    "    \n",
    "    #print(rule_count)\n",
    "    \n",
    "    coref_total_df = SumCoResStats(total_stats, my_book_name)\n",
    "    sieve_total_df = SumSieveStats(total_stats, my_book_name)\n",
    "    \n",
    "    return Mentions, coref_lst, mention_stats_df, coref_stats_df, \\\n",
    "            sieve_stats_df, coref_total_df, sieve_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start.\n",
      "\n",
      " Mention Parse Statistics Psalms: \n",
      " 18599 phrase atoms INPUT \n",
      " 18552 phrase atoms SUCCESFULLY parsed \n",
      " +27 extra mentions SUCCESFULLY parsed from phrase atoms \n",
      " -98 phrase atoms without mentions \n",
      " 47 phrase atom parse ERRORS \n",
      " +0 extra mentions SUCCESFULLY parsed from phrase atom errors \n",
      " -44 phrase atoms without mentions from phrase atom ERRORS \n",
      " 18484 mention coreference input \n",
      " 99.7% parsing succes \n",
      " 0.3% parsing error\n",
      "Parsing and resolution: done.\n",
      "\n",
      " Coreference Resolution Statistics Psalms: \n",
      " 18484 total input corefs \n",
      " 10554 corefs RESOLVED \n",
      " 7930 corefs UNRESOLVED \n",
      " 57.1% corefs RESOLVED \n",
      " 42.9% corefs UNRESOLVED \n",
      " 2647 classes\n",
      "\n",
      " Sieve Statistics Psalms: \n",
      " 1p 2p Pronoun Sieve: 2756 \n",
      " Vocative Sieve: 539 \n",
      " String Sieve: 1757 \n",
      " Entity Sieve: 336 \n",
      " Apposition Sieve: 50 \n",
      " Fronted Element Sieve: 20 \n",
      " Predicate Sieve: 3130 \n",
      " 3p Pronoun Sieve: 1966 \n",
      " Total Sieves: 10554 \n",
      " Total Classes: 2647\n"
     ]
    }
   ],
   "source": [
    "mentions, corefs, mention_stats_df, coref_stats_df,\\\n",
    "        sieve_stats_df, coref_total_df, sieve_total_df = mimi('Psalms', 1, 150, 1) #105, 106, 136 (veel herhaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv, 'v', [])\n",
    "    except getopt.GetoptError:\n",
    "        Usage()\n",
    "    print(len(args), len(argv))\n",
    "    if len(args) == 2:\n",
    "        last_chapter = int(args[1])\n",
    "    elif len(args) == 3:\n",
    "        last_chapter = int(args[2])\n",
    "    else:\n",
    "        Usage()   \n",
    "    \n",
    "    first_chapter = int(args[1])\n",
    "    book_name = args[0]\n",
    "    \n",
    "    mimi(book_name, first_chapter, last_chapter)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
