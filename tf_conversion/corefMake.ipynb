{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix: \n",
    "\n",
    "* When selecting only a few chapters for conversion with `selectChapters = {1, 2}`, all earlier done conversions to TF are overwritten.\n",
    "\n",
    "* When the conversion of all annotations for different books have been done, and a new push is done for one specific book, all annotions are overwritten except for the updated data of one book. \n",
    "\n",
    "Work around for now:\n",
    "\n",
    "* Correct annotations, and push al annotated books one by one again. It just costs a minute, but needs a fix anyway. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"images/tf-small.png\" width=\"90\"/>\n",
    "<img align=\"right\" src=\"images/etcbc.png\" width=\"100\"/>\n",
    "\n",
    "# From Coreference Annotations to Text-Fabric Data\n",
    "\n",
    "The code in this notebook converts the coreference resolution annotations done in brat to Text-Fabric data. Just run the cells, and follow the instructions written in the cells above the code or else in the code which are indicated by a `#`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'erwich/roorda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import rmtree\n",
    "from glob import glob\n",
    "\n",
    "from tf.app import use\n",
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Administration\n",
    "\n",
    "The location on your PC where the annotated data is taken from for the TF conversion has the form: \n",
    "* `{OUTPUT_BASE}/{bookName}/{ANNOTATE}/*.ann`\n",
    "\n",
    "For the standoff files the form of the location is: \n",
    "* `{OUTPUT_BASE}/{bookName}/{STANDOFF}/{fileName}.tsv`\n",
    "\n",
    "The location on your PC where the converted TF data is stored has the form: \n",
    "* `{GITHUB_BASE}/{ORG}/{REPO}/{PATH}/{VERSION}`\n",
    "\n",
    "All the CONSTANTS can be specified in the cell below. It is possible to convert coreference annotations, if existent of course, for any Hebrew Bible book. The book is specified with `bookName` in the function `getFeatures()` below, e.g. `getFeatures('Psalms')`\n",
    "\n",
    "The textual data of the BHSA that form the basis of the coreference annotations has been generated in the fixed 2017 version, so do not change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used the fixed 2017 data, do not change this\n",
    "VERSION = '2017'\n",
    "\n",
    "# Enter your GitHub repo \n",
    "ORG = 'cmerwich' \n",
    "\n",
    "# The locations where you want to store the converted data \n",
    "REPO = 'participant-analysis' \n",
    "PATH = 'coreference/tf'\n",
    "\n",
    "# The output base indicates where the programs below can find the annotated data\n",
    "OUTPUT_BASE = os.path.expanduser('~/Sites/brat/data/coref')\n",
    "\n",
    "# GitHub location on your computer\n",
    "GITHUB_BASE = os.path.expanduser('~/github')\n",
    "\n",
    "# This is the annotation folder \n",
    "ANNOTATE = f'annotate'\n",
    "\n",
    "# The standoff folder is important for the TF conversion \n",
    "STANDOFF = f'standoff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the TF App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = use('bhsa', version=VERSION, hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load consonantal feature\n",
    "\n",
    "`g_cons` is the consonantal representation of a word occurrence in BHSA transliteration. It operates on a word object. The feature is needed for the conversion of the annotation of suffixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.load('g_cons', add=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Chapters for TF Conversion\n",
    "\n",
    "When a certain part of the desired annotations have been done, but not everything yet, it is possible to select a specific number of chapters for the conversion. Just uncomment line 1 in the cell below, and comment line 3. If you want to convert all annotations, just run the cell as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectChapters = {1, 2}\n",
    "\n",
    "selectChapters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Features for TF\n",
    "\n",
    "Three features are made:\n",
    "\n",
    "1. `mention`: contains all referring expressions which consist of NP's, named entities, suffixes, person/gender/number of verbs, personal pronouns and demonstrative pronouns, . \n",
    "2. `mentionNote`: annotator notes that have been stored on the mentions. \n",
    "3. `coref`: the coreference relations between mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeFeatures = dict(\n",
    "    mention={},\n",
    "    mentionNote={},\n",
    "    coref={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookupNode(aStart, standoffInfo, standoffInfoNonFirst):\n",
    "    node = None\n",
    "    isPart = False\n",
    "    if aStart in standoffInfo:\n",
    "        (node, sEnd, sWord) = standoffInfo[aStart]\n",
    "    else:\n",
    "        if aStart in standoffInfoNonFirst:\n",
    "            (node, sEnd, sWord) = standoffInfo[standoffInfoNonFirst[aStart]]\n",
    "            isPart = aStart != sEnd\n",
    "    return (node, isPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Features \n",
    "\n",
    "Specify the Hebrew Bible book that has been annotated in `getFeatures()` below. `getFeatures` assembles all the features that are necessary for a sound TF conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(bookName):\n",
    "    bookBase = f'{OUTPUT_BASE}/{bookName}'\n",
    "    files = glob(f'{bookBase}/{ANNOTATE}/*.ann')\n",
    "    \n",
    "    nChapters = 0\n",
    "    \n",
    "    for annFile in sorted(files):\n",
    "        (directory, fileNameFull) = os.path.split(annFile)\n",
    "        (fileName, ext) = os.path.splitext(fileNameFull)\n",
    "        standoffFile = f'{bookBase}/{STANDOFF}/{fileName}.tsv'\n",
    "        chapter = int(fileName[len(bookName) + 1:].lstrip('0'))\n",
    "        if selectChapters is not None and chapter not in selectChapters:\n",
    "            continue\n",
    "        \n",
    "        standoffInfo = {}\n",
    "        standoffInfoNonFirst = {}\n",
    "        \n",
    "        with open(standoffFile) as fh:\n",
    "            first = True\n",
    "            errors = 0\n",
    "            minPos = None\n",
    "            maxPos = None\n",
    "            for (i, line) in enumerate(fh):\n",
    "                epos = f'{fileName}.tsv{i + 1} - '\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue  # header\n",
    "                (start, end, node, word) = line.rstrip('\\n').split('\\t')\n",
    "                start = int(start)\n",
    "                end = int(end)\n",
    "                node = int(node)\n",
    "                if start == end:  # empty word\n",
    "                    continue\n",
    "                if maxPos is None or end > maxPos:\n",
    "                    maxPos = end\n",
    "                if minPos is None or start < minPos:\n",
    "                    minPos = start\n",
    "                if start in standoffInfo:\n",
    "                    error(f'{epos}{start} for multiple items: {standoffInfo[start]}')\n",
    "                    errors += 1\n",
    "                standoffInfo[start] = (node, end, word)\n",
    "        \n",
    "        currentStart = None\n",
    "        \n",
    "        for p in range(minPos, maxPos + 1):\n",
    "            if p in standoffInfo:\n",
    "                currentStart = p\n",
    "            else:\n",
    "                standoffInfoNonFirst[p] = currentStart\n",
    "        \n",
    "        if errors:\n",
    "            error(f'{book} {chapter}: {errors} errors in standoff file')\n",
    "        \n",
    "        errors = 0\n",
    "        \n",
    "        mention = {}\n",
    "        mentionNote = {}\n",
    "        coref = {}\n",
    "        \n",
    "        firstChars = {'T', '#', '*'}\n",
    "        cClass = 0\n",
    "        \n",
    "        with open(annFile) as fh:\n",
    "            for (i, line) in enumerate(fh):\n",
    "                epos = f'{fileName}.tsv:{i + 1} - '\n",
    "                line = line.rstrip('\\n')\n",
    "                firstChar = line[0]\n",
    "                \n",
    "                if firstChar not in firstChars:\n",
    "                    error(f'{epos}Unrecognized line \"{line}\"')\n",
    "                    errors +=1\n",
    "                    continue\n",
    "                    \n",
    "                numFields = 2 if firstChar =='*' else 3\n",
    "                parts = line.split('\\t')\n",
    "                \n",
    "                if len(parts) != numFields:\n",
    "                    error(f'{epos}line does not have exactly {numFields} parts: \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                        \n",
    "                if firstChar == 'T':\n",
    "                    (tPart, mentionStr, aWord) = parts\n",
    "                    mParts = mentionStr.split()\n",
    "                    if len(mParts) != 3:\n",
    "                        error(f'{epos}T-line mention does not have exactly 3 parts: \"{line}\"')\n",
    "                        errors += 1\n",
    "                        continue\n",
    "                    (mm, aStart, aEnd) = mParts\n",
    "                    aStart = int(aStart)\n",
    "                    aEnd = int(aEnd)\n",
    "                    \n",
    "                    (nodeStart, isPartStart) = lookupNode(aStart, standoffInfo, standoffInfoNonFirst)\n",
    "                    if nodeStart is None:\n",
    "                        error(f'{epos}Mention start position not found in standoff file \"{line}\"')\n",
    "                        errors += 1\n",
    "                        continue\n",
    "\n",
    "                    (nodeEnd, isPartEnd) = lookupNode(aEnd, standoffInfo, standoffInfoNonFirst)\n",
    "                    if nodeEnd is None:\n",
    "                        error(f'{epos}Mention end position not found in standoff file \"{line}\"')\n",
    "                        errors += 1\n",
    "                        continue\n",
    "                    wordSize = nodeEnd - nodeStart + 1\n",
    "                    wordPart = aWord if isPartStart or isPartEnd else None\n",
    "                            \n",
    "                    mention.setdefault(nodeStart, []).append((tPart, wordSize, wordPart))\n",
    "                        \n",
    "                elif firstChar == '#':\n",
    "                    (code, spec, note) = parts\n",
    "                    sParts = spec.split()\n",
    "                    if len(sParts) != 2:\n",
    "                        error(f'{epos}#-line spec does not have exactly 2 parts: \"{line}\"')\n",
    "                        errors += 1\n",
    "                        continue\n",
    "                    tPart = sParts[1]\n",
    "                    mentionNote.setdefault(tPart, set()).add(note)\n",
    "                elif firstChar == '*':\n",
    "                    (char, data) = parts\n",
    "                    dataParts = data.split()\n",
    "                    if len(dataParts) <= 1 or dataParts[0] != 'Coreference':\n",
    "                        error(f'{epos}*-line spec does not have the right parts: \"{line}\"')\n",
    "                        errors += 1\n",
    "                        continue\n",
    "                    cClass += 1\n",
    "                    for tPart in dataParts[1:]:\n",
    "                        if tPart in coref:\n",
    "                            error(f'{epos}*-\"{tPart} occurs in multiple classes \"{coref[tPart]}\" in \"{line}\"')\n",
    "                            errors += 1\n",
    "                            continue\n",
    "                        coref[tPart] = f'C{cClass}'\n",
    "                    \n",
    "        if errors:\n",
    "            error(f'{book} {chapter}: {errors} errors in annotation file')\n",
    "        else:\n",
    "            info('.', tm=False, nl=False)\n",
    "        \n",
    "        for (node, parts) in mention.items():\n",
    "            parts = sorted(\n",
    "                (x for x in parts),\n",
    "                key=lambda x: ('' if x[2] is None else x[2], x[1], x[0])\n",
    "            )\n",
    "            valuesM = []\n",
    "            valuesC = []\n",
    "            notes = set()\n",
    "\n",
    "            for (tPart, wordSize, wordPart) in parts:\n",
    "                cPart = coref.get(tPart, tPart)\n",
    "\n",
    "                wordSize = str(wordSize) if wordSize > 1 else ''\n",
    "                wordPart = wordPart or ''\n",
    "\n",
    "                isSuffix = 's' if wordPart and F.g_cons.v(node).endswith(wordPart) else ''\n",
    "                \n",
    "                valueM = (tPart[0], tPart[1:], wordSize, isSuffix, wordPart)\n",
    "                valueC = (cPart[0], cPart[1:], wordSize, isSuffix, wordPart)\n",
    "                \n",
    "                valuesM.append(','.join(valueM))\n",
    "                valuesC.append(','.join(valueC))\n",
    "                \n",
    "                if tPart in mentionNote:\n",
    "                    notes |= mentionNote[tPart]\n",
    "                         \n",
    "            nodeFeatures['mention'][node] = '|'.join(valuesM)            \n",
    "            nodeFeatures['coref'][node] = '|'.join(valuesC)\n",
    "            if notes:\n",
    "                nodeFeatures['mentionNote'][node] = '|'.join(sorted(notes))\n",
    "                \n",
    "        nChapters += 1\n",
    "                \n",
    "    info('', tm=False)\n",
    "    info(f'Done assembling features of {nChapters} chapters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeatures('Numbers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Metadeta\n",
    "\n",
    "Before the actual conversion takes place first the metadata need to be specified below. Since I have annotated the Psalms data myself, I am the only author. Of course it is possible to specify multiple authors or annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = {\n",
    "    '': dict(\n",
    "            title='Participant analysis',\n",
    "            author='Christiaan Erwich',\n",
    "    ),\n",
    "    'mention': dict(\n",
    "        valueType='str',\n",
    "        description='mention annotations made through Brat',\n",
    "        explanation='the analysis is per chapter',\n",
    "    ),\n",
    "    'mentionNote': dict(\n",
    "        valueType='str',\n",
    "        description='comments on mention annotations made through Brat',\n",
    "        explanation='the analysis is per chapter',\n",
    "    ),\n",
    "    'coref': dict(\n",
    "        valueType='str',\n",
    "        description='coreference equivalence class of mention annotations made through Brat',\n",
    "        explanation='the analysis is per chapter',\n",
    "    ),\n",
    "}\n",
    "\n",
    "edgeFeatures = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert \n",
    "\n",
    "The new features are stored in my github repository as can be seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFW = Fabric(locations=f'{GITHUB_BASE}/{ORG}/{REPO}/{PATH}/{VERSION}')\n",
    "TFW.save(\n",
    "    nodeFeatures=nodeFeatures,\n",
    "    edgeFeatures=edgeFeatures,\n",
    "    metaData=metaData,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
