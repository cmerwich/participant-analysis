{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twee versie tabellen \n",
    "\n",
    "ANN\n",
    "- (+) trailers\n",
    "- (+) '_|'\n",
    "- trailer bestaat uit 2 cijfers achter elkaar op voorwaarde dat je al een spatie hebt gezien op die regel, een verschil van 2\n",
    "- zelfde geldt voor '_|' elke keer dar je dit verschijnsel tegenkomt is er een verschil van 2\n",
    "i = 0\n",
    "ann_ann = []\n",
    "for line in file:\n",
    "    lijn lezen tot na eerste spatie (versnummer en spatie)\n",
    "    for character in line:\n",
    "        i += 1\n",
    "        if character in '[0-9]{2}|_\\|': (regex inzetten: )\n",
    "            ann_ann.append(i)\n",
    "\n",
    "MiMi\n",
    "- (-) trailers\n",
    "- (+) hypens en plussen \n",
    "- bestaat ui 1 positie\n",
    "i = 0\n",
    "ann_mimi = []\n",
    "for line in file:\n",
    "    for character in line:\n",
    "        i += 1\n",
    "        if character in {+, -}:\n",
    "            ann_mimi.append(i)\n",
    "            \n",
    "\n",
    "Aanpak\n",
    "- Iteren over elke Psalm, haal ik karakters uit, sla de index van het karakter op\n",
    "- 2 index lijsten maken: ann_ann_list, ann_mimi_list\n",
    "- ann_ann samenvoegen met ann_mimi in position_dict. position_dict is per chapter, dan leegmaken\n",
    "- vertalen van oud (ann) naar nieuw (mimi). \n",
    "- op alle posities waar er iets verspringt, in de nieuwe of oude file, heb je een entry in position_dict\n",
    "- de verspringpunten zijn de keys; het aantal karakters dat het verschilt de values\n",
    "- vb voor Psalm 1: index 7 : int(1) (h-isj), voor mimifile '-' +1 \n",
    "                   index 14 : int(-1) asjer05, voor annfile '05' -2 \n",
    "                   \n",
    "- iaa mention dict vullen met aangepaste mention strings die indices van annotaties bevatten 'mention 1 5', begin index 1 en eind index 5 dus apart aanpassen\n",
    "- mention vertaler functie: def translate_mention()\n",
    "- binnen translate_mention(), functie: def translate_index() die begin en eind index aanpast, gebruik daar voor Reindex() uit declust.py\n",
    "\n",
    "PS_A   = $(HOME)/Sites/brat/data/coref/Psalms/annotate\n",
    "PS_B   = $(HOME)/github/cmerwich/participant-analysis/mimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'erwich/sikkel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getopt\n",
    "import os, sys\n",
    "from sys import argv, exit, stderr\n",
    "import re\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "from operator import attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = re.compile('\\&|[0-9]{2}|_\\|')\n",
    "# = re.compile('[0-9]{1,3}[\\s]') # match verses\n",
    "\n",
    "PATTERN = re.compile('[0-9]{2}|_\\|')\n",
    "TAB = '\\t'\n",
    "NL = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mention:\n",
    "    def __init__(self, name, start=0, end=0, lex='', note='', file=0):\n",
    "        self.name = name    # Identifier of the mention, e.g. T32\n",
    "        self.start = start  # Start of the position in the file\n",
    "        self.end = end      # End of the position in the file\n",
    "        self.lex = lex      # Lexical information\n",
    "        self.note = note    # AnnotatorNotes\n",
    "        self.file = file    # Index of the file to which it is to be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(*args, **kwargs):\n",
    "    print(*args, file=stderr, **kwargs)\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Usage():\n",
    "    stderr.write('usage: declust -a input.ann input.txt...\\n')\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOffsetsAnn(chapter_text):\n",
    "    \n",
    "    i = 0\n",
    "    ann_txt_list = []\n",
    "    with open(chapter_text) as fh:\n",
    "        for (ln, line) in enumerate(fh):\n",
    "            match = PATTERN.finditer(line)\n",
    "            for character in line:\n",
    "                i += 1\n",
    "                for m in match:\n",
    "                    # filter out psalms title on line 0, \n",
    "                    # and (0, 2) for verse numbers with two digits or more\n",
    "                    if ln != 0 and m.span() != (0, 2):\n",
    "                        #ann_txt_list.append(f'{i+m.start()} {i+m.start()+1}') #{m.end()}\n",
    "                        ann_txt_list.append(i+m.start())\n",
    "                                          \n",
    "    return ann_txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GetOffsetsMimi(chapter_text):\n",
    "    \n",
    "    i = 0\n",
    "    mimi_txt_list = []\n",
    "    \n",
    "    with open(chapter_text) as fh:\n",
    "        for (ln, line) in enumerate(fh):\n",
    "            firstChar = line[0]\n",
    "            for character in line: \n",
    "                #print(character, i)\n",
    "                i += 1\n",
    "                if character in {'+', '-'}:\n",
    "                    mimi_txt_list.append(i-1)\n",
    "               \n",
    "    return mimi_txt_list         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(ann_txt_list, mimi_txt_list):\n",
    "    d = {}\n",
    "    for i in ann_txt_list:\n",
    "        d[i] = -2\n",
    "    for e in mimi_txt_list:\n",
    "        d[e] = 1\n",
    "    od = OrderedDict(sorted(d.items()))\n",
    "    return od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MakeOffsets(od):\n",
    "    offset = 0\n",
    "    jumps_list = []\n",
    "    for k, v in od.items():\n",
    "        point = k + (offset if v == 1 else 0)\n",
    "        offset += v\n",
    "        jumps_list.append((point, offset))\n",
    "    return jumps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(jumps_list, coor):\n",
    "    i = 0\n",
    "    while jumps_list[i+1][0] < coor:\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def TranslateIndex(jumps_list, mentions): \n",
    "    for m in mentions: \n",
    "        i = find_index(jumps_list, m.start)\n",
    "        print('i: ', i, 't0 ', jumps_list[i][0], 't1 ', jumps_list[i][1])\n",
    "        if m.start == jumps_list[i][0]:\n",
    "            m.start = m.start + jumps_list[i][1]\n",
    "            m.end = m.end + jumps_list[i][1]\n",
    "\n",
    "def TranslateIndex(jumps_list, coor): \n",
    "    i = find_index(jumps_list, coor)\n",
    "    if coor == jumps_list[i][0]:\n",
    "        ni = coor + jumps_list[i][1]\n",
    "        \n",
    "def TranslateIndex(jumps_list, m_start, m_end): \n",
    "    i = find_index(jumps_list, m_start)\n",
    "    print('i: ', i, 't0 ', jumps_list[i][0], 't1 ', jumps_list[i][1])\n",
    "    \n",
    "    return m_start + jumps_list[i][1], m_end + jumps_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranslateIndex(jumps_list, mentions): \n",
    "    for m in mentions: \n",
    "        i = find_index(jumps_list, m.start)\n",
    "        print('i: ', i, 't0 ', jumps_list[i][0], 't1 ', jumps_list[i][1])\n",
    "        if m.start == jumps_list[i][0]:\n",
    "            m.start = m.start + jumps_list[i][1]\n",
    "            m.end = m.end + jumps_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse(ann_file, jumps_list):\n",
    "    '''\n",
    "    Parses a plain text annotation file per Bible Book chapter \n",
    "    (e.g. Psalms 001) that has been annotated for coreference information.\n",
    "    Returns a list of lists of coref-mention objects and \n",
    "    a list of mention objects. \n",
    "    '''\n",
    "    \n",
    "    errors = 0\n",
    "    t2mDict = {}\n",
    "    Mentions = []\n",
    "    Corefs = []\n",
    "    MentionIndexDict = {}\n",
    "    mentionNote = {}\n",
    "    dataPartsList = []\n",
    "\n",
    "    firstChars = {'T', '#', '*'}\n",
    "    cClass = 0\n",
    "\n",
    "    with open(ann_file) as fh:\n",
    "        for (i, line) in enumerate(fh):\n",
    "            epos = f'{i + 1} '\n",
    "            line = line.rstrip('\\n')\n",
    "            firstChar = line[0]\n",
    "\n",
    "            if firstChar not in firstChars:\n",
    "                error(f'{epos}Unrecognized line \"{line}\"')\n",
    "                errors +=1\n",
    "                continue\n",
    "\n",
    "            numFields = 2 if firstChar =='*' else 3\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            if len(parts) != numFields:\n",
    "                error(f'{epos}line does not have exactly {numFields} parts: \"{line}\"')\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            if firstChar == 'T':\n",
    "                (tPart, mentionStr, aWord) = parts\n",
    "                mParts = mentionStr.split()\n",
    "                if len(mParts) != 3:\n",
    "                    error(f'{epos}T-line mention does not have exactly 3 parts: \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                (mm, aStart, aEnd) = mParts\n",
    "                start = int(aStart)\n",
    "                end = int(aEnd)\n",
    "                # adjust mention start and end indices for iaa analysis \n",
    "                #theStart, theEnd = TranslateIndex(jumps_list, start, end)\n",
    "                #theStart = TranslateIndex(jumps_list, start)\n",
    "                #theEnd = TranslateIndex(jumps_list, end)\n",
    "                t2mDict[tPart] = mentionStr\n",
    "                m = Mention(tPart, start, end, aWord)\n",
    "                MentionIndexDict[tPart] = len(Mentions)\n",
    "                Mentions.append(m)\n",
    "                \n",
    "            elif firstChar == '*':\n",
    "                corefSets = set()\n",
    "                (char, data) = parts\n",
    "                dataParts = data.split()\n",
    "                if len(dataParts) <= 1 or dataParts[0] != 'Coreference':\n",
    "                    error(f'{epos}*-line spec does not have the right parts: \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                cClass += 1\n",
    "                dataPartsList.append(dataParts)\n",
    "            \n",
    "            elif firstChar == '#':\n",
    "                (code, spec, note) = parts\n",
    "                sParts = spec.split()\n",
    "                if len(sParts) != 2:\n",
    "                    error(f'{epos}#-line spec does not have exactly 2 parts: \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                tPart = sParts[1]\n",
    "                if tPart in MentionIndexDict:\n",
    "                    Mentions[MentionIndexDict[tPart]].note = note\n",
    "                else:\n",
    "                    error(f'Annotatornote \"{note}\": {tPart}: Mention not found')\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                mentionNote.setdefault(tPart, set()).add(note)\n",
    "                   \n",
    "        for l in dataPartsList:\n",
    "            corefSets = set() # for error analysis \n",
    "            corefLists = []\n",
    "            for tPart in l[1:]:\n",
    "                if tPart in corefSets:\n",
    "                    error(f'{epos}*-\"{tPart} occurs in multiple classes \"{corefSets[tPart]}\" in \"{line}\"')\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                corefSets.add(t2mDict[tPart]) # for error analysis \n",
    "                if tPart in MentionIndexDict:\n",
    "                    corefLists.append(Mentions[MentionIndexDict[tPart]])\n",
    "                else:\n",
    "                    error(f'Coref: {tPart}: Mention not found')\n",
    "                    sys.exit(1)\n",
    "            Corefs.append(corefLists) # list of lists, one list is one coreference class. \n",
    "            \n",
    "    if errors:\n",
    "        error(f'There are {errors} errors in annotation file(s)')\n",
    "    \n",
    "    return Mentions, Corefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenAnn(path):\n",
    "    '''\n",
    "    Opens an annotation file with extension `.ann' for each chapter\n",
    "    in current folder. \n",
    "    '''\n",
    "    \n",
    "    filename_w_ext = os.path.basename(path)\n",
    "    filename, file_extension = os.path.splitext(filename_w_ext)\n",
    "    filename_ann = f'{filename}_n{file_extension}'\n",
    "    ann_file = open(filename_ann, 'w')\n",
    "    \n",
    "    return ann_file\n",
    "\n",
    "#OpenAnn('/Users/Christiaan/Sites/brat/data/coref/Psalms/annotate/Psalms_001.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteMentions(mentions, ann_file):\n",
    "    '''\n",
    "    Writes the mentions and annotatornotes on mentions to the corresponding ann file. \n",
    "    '''\n",
    "    \n",
    "    i = 0\n",
    "    sorted_mentions = sorted(mentions, key=attrgetter('start'))\n",
    "    for m in sorted_mentions:\n",
    "        ann_file.write(f'{m.name}{TAB}Mention {str(m.start)} {str(m.end)}{TAB}{m.lex}{NL}')\n",
    "        \n",
    "        if m.note != '':\n",
    "            i += 1\n",
    "            ann_file.write(f'#{str(i)}{TAB}AnnotatorNotes {m.name}{TAB}{m.note}{NL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteCorefs(corefs, ann_file):\n",
    "    '''\n",
    "    Writes a coref class with mentions to .ann file object.\n",
    "    '''\n",
    "    \n",
    "    for c in corefs:\n",
    "        ann_file.write(f'*{TAB}Coreference')\n",
    "        sorted_corefs = sorted(c, key=attrgetter('start'))\n",
    "        for m in sorted_corefs:\n",
    "            ann_file.write(f' {m.name}') \n",
    "        ann_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Close(file): \n",
    "    '''\n",
    "    Close ann file object.\n",
    "    '''\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Translate(ann_txt, mimi_txt, ann_file):\n",
    "    # ann_txt: path to ann txt files\n",
    "    # mimi_txt = path to mimi txt files \n",
    "    # ann_file = path to (chris) ann_file\n",
    "    \n",
    "    ann_offsets = GetOffsetsAnn(ann_txt)\n",
    "    mimi_offsets = GetOffsetsMimi(mimi_txt)\n",
    "    ordered_dict = Merge(ann_offsets, mimi_offsets)\n",
    "    jumps_list = MakeOffsets(ordered_dict)\n",
    "    pprint(jumps_list)\n",
    "    Mentions, Corefs = Parse(ann_file, jumps_list)\n",
    "    TranslateIndex(jumps_list, Mentions)\n",
    "    new_ann_file = OpenAnn(ann_file)\n",
    "    WriteMentions(Mentions, new_ann_file)\n",
    "    WriteCorefs(Corefs, new_ann_file)\n",
    "    Close(new_ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(28, 1),\n",
      " (43, -1),\n",
      " (55, -3),\n",
      " (61, -2),\n",
      " (89, -1),\n",
      " (98, 0),\n",
      " (102, -2),\n",
      " (119, -1),\n",
      " (125, 0),\n",
      " (137, 1),\n",
      " (145, 2),\n",
      " (146, 0),\n",
      " (157, 1),\n",
      " (159, 2),\n",
      " (182, 3),\n",
      " (181, 1),\n",
      " (199, 2),\n",
      " (205, 3),\n",
      " (209, 4),\n",
      " (214, 5),\n",
      " (217, 6),\n",
      " (223, 7),\n",
      " (220, 5),\n",
      " (229, 6),\n",
      " (238, 7),\n",
      " (253, 8),\n",
      " (257, 6),\n",
      " (269, 7),\n",
      " (300, 8),\n",
      " (305, 9),\n",
      " (318, 10),\n",
      " (311, 8),\n",
      " (327, 9),\n",
      " (341, 10),\n",
      " (346, 11),\n",
      " (362, 12),\n",
      " (367, 13),\n",
      " (358, 11),\n",
      " (371, 12),\n",
      " (390, 13),\n",
      " (395, 14),\n",
      " (407, 15),\n",
      " (394, 13),\n",
      " (420, 14),\n",
      " (429, 15),\n",
      " (433, 13),\n",
      " (479, 14),\n",
      " (470, 12),\n",
      " (485, 13),\n",
      " (493, 14),\n",
      " (492, 12),\n",
      " (519, 13),\n",
      " (540, 14),\n",
      " (548, 15),\n",
      " (538, 13),\n",
      " (564, 14)]\n",
      "i:  0 t0  28 t1  1\n",
      "i:  0 t0  28 t1  1\n",
      "i:  0 t0  28 t1  1\n",
      "i:  0 t0  28 t1  1\n",
      "i:  0 t0  28 t1  1\n",
      "i:  1 t0  43 t1  -1\n",
      "i:  2 t0  55 t1  -3\n",
      "i:  4 t0  89 t1  -1\n",
      "i:  3 t0  61 t1  -2\n",
      "i:  5 t0  98 t1  0\n",
      "i:  6 t0  102 t1  -2\n",
      "i:  9 t0  137 t1  1\n",
      "i:  11 t0  146 t1  0\n",
      "i:  11 t0  146 t1  0\n",
      "i:  13 t0  159 t1  2\n",
      "i:  13 t0  159 t1  2\n",
      "i:  13 t0  159 t1  2\n",
      "i:  13 t0  159 t1  2\n",
      "i:  15 t0  181 t1  1\n",
      "i:  15 t0  181 t1  1\n",
      "i:  16 t0  199 t1  2\n",
      "i:  16 t0  199 t1  2\n",
      "i:  17 t0  205 t1  3\n",
      "i:  18 t0  209 t1  4\n",
      "i:  18 t0  209 t1  4\n",
      "i:  19 t0  214 t1  5\n",
      "i:  22 t0  220 t1  5\n",
      "i:  23 t0  229 t1  6\n",
      "i:  23 t0  229 t1  6\n",
      "i:  24 t0  238 t1  7\n",
      "i:  24 t0  238 t1  7\n",
      "i:  24 t0  238 t1  7\n",
      "i:  25 t0  253 t1  8\n",
      "i:  26 t0  257 t1  6\n",
      "i:  27 t0  269 t1  7\n",
      "i:  27 t0  269 t1  7\n",
      "i:  27 t0  269 t1  7\n",
      "i:  27 t0  269 t1  7\n",
      "i:  27 t0  269 t1  7\n",
      "i:  27 t0  269 t1  7\n",
      "i:  27 t0  269 t1  7\n",
      "i:  28 t0  300 t1  8\n",
      "i:  29 t0  305 t1  9\n",
      "i:  27 t0  269 t1  7\n",
      "i:  29 t0  305 t1  9\n",
      "i:  31 t0  311 t1  8\n",
      "i:  32 t0  327 t1  9\n",
      "i:  32 t0  327 t1  9\n",
      "i:  32 t0  327 t1  9\n",
      "i:  33 t0  341 t1  10\n",
      "i:  34 t0  346 t1  11\n",
      "i:  35 t0  362 t1  12\n",
      "i:  35 t0  362 t1  12\n",
      "i:  37 t0  358 t1  11\n",
      "i:  38 t0  371 t1  12\n",
      "i:  39 t0  390 t1  13\n",
      "i:  40 t0  395 t1  14\n",
      "i:  42 t0  394 t1  13\n",
      "i:  42 t0  394 t1  13\n",
      "i:  43 t0  420 t1  14\n",
      "i:  45 t0  433 t1  13\n",
      "i:  45 t0  433 t1  13\n",
      "i:  45 t0  433 t1  13\n",
      "i:  45 t0  433 t1  13\n",
      "i:  45 t0  433 t1  13\n",
      "i:  45 t0  433 t1  13\n",
      "i:  47 t0  470 t1  12\n",
      "i:  48 t0  485 t1  13\n",
      "i:  50 t0  492 t1  12\n",
      "i:  50 t0  492 t1  12\n",
      "i:  50 t0  492 t1  12\n",
      "i:  51 t0  519 t1  13\n",
      "i:  51 t0  519 t1  13\n",
      "i:  51 t0  519 t1  13\n",
      "i:  3 t0  61 t1  -2\n",
      "i:  3 t0  61 t1  -2\n",
      "i:  31 t0  311 t1  8\n",
      "i:  34 t0  346 t1  11\n",
      "i:  38 t0  371 t1  12\n",
      "i:  50 t0  492 t1  12\n",
      "i:  51 t0  519 t1  13\n",
      "i:  6 t0  102 t1  -2\n",
      "i:  8 t0  125 t1  0\n",
      "i:  8 t0  125 t1  0\n",
      "i:  7 t0  119 t1  -1\n",
      "i:  9 t0  137 t1  1\n"
     ]
    }
   ],
   "source": [
    "Translate('Psalms_002-ann.txt', 'Psalms_002-mimi.txt', 'Psalms_002.ann')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
